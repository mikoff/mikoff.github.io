<!doctype html><html lang=en><head><title>Notes on Computer vision · Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Notes on Computer vision Link to heading I have made these notes while reading Computer vision: Models, Learning, and Inference book.
Coordinate systems notation Link to heading $\mathbf{R}_{wc}$ is a rotation matrix such that, after its application to the camera axes, they become collinear with the world axes. Some describe it as a matrix that rotates a vector from the camera coordinate system to the world coordinate system. However, this description can be slightly misleading, as vectors exist in space and are not physically rotated."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on Computer vision"><meta name=twitter:description content="Notes on Computer vision Link to heading I have made these notes while reading Computer vision: Models, Learning, and Inference book.
Coordinate systems notation Link to heading $\mathbf{R}_{wc}$ is a rotation matrix such that, after its application to the camera axes, they become collinear with the world axes. Some describe it as a matrix that rotates a vector from the camera coordinate system to the world coordinate system. However, this description can be slightly misleading, as vectors exist in space and are not physically rotated."><meta property="og:title" content="Notes on Computer vision"><meta property="og:description" content="Notes on Computer vision Link to heading I have made these notes while reading Computer vision: Models, Learning, and Inference book.
Coordinate systems notation Link to heading $\mathbf{R}_{wc}$ is a rotation matrix such that, after its application to the camera axes, they become collinear with the world axes. Some describe it as a matrix that rotates a vector from the camera coordinate system to the world coordinate system. However, this description can be slightly misleading, as vectors exist in space and are not physically rotated."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/notes/computer-vision-notes/"><meta property="article:section" content="notes"><meta property="article:published_time" content="2024-03-24T12:00:00+03:00"><meta property="article:modified_time" content="2024-03-24T12:00:00+03:00"><link rel=canonical href=https://mikoff.github.io/notes/computer-vision-notes/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li><li class=navigation-item><a class=navigation-link href=/notes/>Notes</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=https://mikoff.github.io/notes/computer-vision-notes/>Notes on Computer vision</a></h1></header><h1 id=notes-on-computer-vision>Notes on Computer vision
<a class=heading-link href=#notes-on-computer-vision><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I have made these notes while reading <a href=http://www.computervisionmodels.com/ class=external-link target=_blank rel=noopener>Computer vision: Models, Learning, and Inference</a> book.</p><h2 id=coordinate-systems-notation>Coordinate systems notation
<a class=heading-link href=#coordinate-systems-notation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>$\mathbf{R}_{wc}$ is a rotation matrix such that, after its application to the camera axes, they become collinear with the world axes. Some describe it as a matrix that rotates a vector from the camera coordinate system to the world coordinate system. However, this description can be slightly misleading, as vectors exist in space and are not physically rotated.
$\mathbf{t}_{wc}$ is a translation vector from the origin of the world axes to the origin of the camera axes; if we apply this translation to the point associated with the origin of the world coordinate system, it will coincide with the point associated with the origin of the camera coordinate system.</p><p>Now, if we want to determine the position of a point known in the camera coordinate system in terms of the world coordinate system, we can apply the following transformation:
$\mathbf{p}_w = \mathbf{R}_{wc} \mathbf{p}_c + \mathbf{t}_{wc}$.</p><p>Also, note that $\mathbf{t}_{wc} = -\mathbf{R}_{wc}\mathbf{t}_{cw}$ (can be easily derived from the equations describing the $(\mathbf{R}_{cw}, \mathbf{t}_{cw})$ and $(\mathbf{R}_{wc}, \mathbf{t}_{wc})$ transformations applied to two points).</p><p>Because of the vector addition the aforementioned transformation expression is not linear, also stacking these transformation becomes a tedious task. So it is more useful to combine both Rotational and translation parts into one matrix, this matrix allows to work on homogeneous coordinates (we transform 3D vector to homogeneous one by adding one element at the end). Then such a vector could be transformed using transformation matrix $\mathbf{T}_{wc}$. This matrix has a special form and set of all these matrices is known as $SE(3)$ group.</p><h2 id=transformation-types>Transformation types
<a class=heading-link href=#transformation-types><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Most common transformations types</p><table><thead><tr><th>Transform</th><th>Matrix</th><th>DoF</th><th>Invariance</th><th><code>Eigen::</code> class</th></tr></thead><tbody><tr><td>Euclidian</td><td>$$\begin{bmatrix}\mathbf{R} & \mathbf{t} \\ \mathbf{0}^T & 1\end{bmatrix}$$</td><td>6</td><td>Length, angle, volume</td><td><code>Matrix3d</code>, <code>AngleAxisd</code></td></tr><tr><td>Similarity</td><td>$$\begin{bmatrix}s\mathbf{R} & \mathbf{t} \\ \mathbf{0}^T & 1\end{bmatrix}$$</td><td>7</td><td>angle, volume ratio</td><td><code>Isometry3d</code></td></tr><tr><td>Affine</td><td>$$\begin{bmatrix}\mathbf{A} & \mathbf{t} \\ \mathbf{0}^T & 1\end{bmatrix}$$</td><td>12</td><td>parallelism, volume ratio</td><td><code>Affine3d</code></td></tr><tr><td>Perspective</td><td>$$\begin{bmatrix}\mathbf{A} & \mathbf{t} \\ \mathbf{a}^T & v\end{bmatrix}$$</td><td>15</td><td>intersections, tangency</td><td><code>Projective3d</code></td></tr></tbody></table><h2 id=camera-model>Camera model
<a class=heading-link href=#camera-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=key-terms>Key terms
<a class=heading-link href=#key-terms><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><strong>Pinhole camera model</strong> - closed chamber with a small hole, rays from an object pass through the hole and form an inverted image. To avoid inversion the <strong>virtual</strong> image is usually used, that would result from placing an image between the pinhole and the object.</p><p><strong>Pinhole model</strong> - generative model that describes the likelihood $Pr(\mathbf{x}|\mathbf{w})$ of observing a feature at position $\mathbf{x} = [x, y]^T$ in the image given that it is the projection of a 3D point</p>$$\mathbf{w} = [u, v, w]^T$$<p>in the the world.</p><p><strong>Principal point</strong> - point where optical axis of the camera strikes the image plane.</p><p><strong>Focal length</strong> - distance between optical center of the camera and principal point.</p><p><strong>Perspective projection</strong> - the process of finding the position of the point, expressed in world coordinate frame, on the image plane. The usual way to do so is to connect a ray between $\mathbf{w}$ and the optical center, the image position $\mathbf{x}$ can be found by observing where this ray strikes the image plane.</p><p><strong>Normalized camera</strong> - the camera, which focal length is one, and the origin of the 2D image coordinate system is at the principal point. In the normalized camera, $\mathbf{w}$ is projected into the image at $\mathbf{x}$ using the following relations:</p>$$
\begin{equation}
\begin{split}
x &= \frac{u}{w}\\
y &= \frac{v}{w},
\end{split}
\end{equation}
$$<p>and all values are measured in the same real-world units. If the camera coordinates are multiplied by any non-zero number, the normalized coordinates are the same. This means that the depth information is lost during the projection process.</p><p>The defined model assumes that the image coordinate center is located at the principal point. Quite often, however, the origin should be at the top-left of the image. To account for this, the model enriches with the parameters $\delta_x$ and $\delta_y$. They are the offsets, or positions of the principal point in pixels, measured from top-left corner of the image.
Another parameter is skew, $\gamma$, it has no clear physical interpretation, but can help to explain the projection of points. The resulting camera model is</p>$$
\begin{equation}
\begin{split}
x &= \frac{\phi_x u + \gamma v}{w} + \delta_x\\
y &= \frac{\phi_y v}{w} + \delta_y.
\end{split}
\end{equation}
$$<h3 id=full-pinhole-camera-model>Full pinhole camera model
<a class=heading-link href=#full-pinhole-camera-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The standard way is to separate the model parameters into two sets:</p><ol><li>intrinsic, or camera parameters: $\{\phi_x, \phi_y, \gamma, \delta_x, \delta_y\}$, that describe camera itself.</li><li>extrinsic parameters: $\{ \boldsymbol{\Omega}, \boldsymbol{\tau} \}$, or $\{ \mathbf{R}_{cw}, \mathbf{t}_{cw} \}$ to express the world points in the coordinate system of the camera before they are passed through the projection model.</li></ol><p>Usually the intrinsic parameters are gathered in intrinsic matrix $\boldsymbol{\Lambda}$:</p>$$
\boldsymbol{\Lambda} = \begin{bmatrix}
\phi_x & \gamma & \delta_x \\
0 & \phi_y & \delta_y\\
0 & 0 & 1
\end{bmatrix},
$$<p>then the full pinhole camera model can be abbreviated as:</p>$$
\mathbf{x} = \text{pinhole}[\mathbf{w}, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}].
$$<p>To account for uncertainty when the estimated position of the feature differs from our predictions, we can use spherically isotropic noise:</p>$$
Pr(\mathbf{x}|\mathbf{w}, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}) = \text{Norm}_{\mathbf{x}}[\text{pinhole}[\mathbf{w}, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}], \sigma^2\mathbf{I}].
$$<p>It is <em>generative</em> model, that describes the likelihood of observing a 2D image point $\mathbf{x}$ given the 3D world point $\mathbf{w}$ and the parameters $\{\boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}\}$.</p><h2 id=distortion>Distortion
<a class=heading-link href=#distortion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To get a larger FoV one usually adds a lens in front of the camera. Usually there are two types of distortions: first comes because of the lens shape and is called <em>radial</em> distortion, second one is the result of misalignment between the lens plane and image surface.</p><h3 id=radial>Radial
<a class=heading-link href=#radial><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>It is the nonlinear warping of the image that depends on the distance from the center of the image. U
Radial distortion is modeled as a polynomial function of the distance from the center of the image. The final positions are expressed as functions of the original ones:</p>$$
\begin{split}
x' = x(1 + \beta_1 r^2 + \beta_2 r^4) \\
y' = y(1 + \beta_1 r^2 + \beta_2 r^4).
\end{split}
$$<p>This distortion is implemented after perspective projection (division by $w$) but before the effect of the intrinsic parameters (focal length, offset, etc.).</p><h3 id=undistortion>Undistortion
<a class=heading-link href=#undistortion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>To make undistortion two approaches could be used:</p><ol><li>We go through the undistorted image pixels and calculate its <em>distorted</em> position, copying the pixel from distorted image to an undistorted one. We can directly use the aforementioned equation.</li><li>We know the positions of distorted pixels and want to find their positions on an undistorted image. To do so we need numerically find the solution of the aforementioned equations.</li></ol><h2 id=three-geometric-problems>Three geometric problems
<a class=heading-link href=#three-geometric-problems><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>These three problems is the an extraction from [2, pp. 304-306].</p><h3 id=problem-1-learning-extrinsic-parameters>Problem 1: Learning extrinsic parameters
<a class=heading-link href=#problem-1-learning-extrinsic-parameters><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><strong>Problem</strong>: recover the position and orientation of the camera relative to a known scene. This is known as the <em>perspective-n-point (PnP)</em> problem or the <em>exterior orientation</em> problem.</p><p>We are given a set of $I$ distinct 3D points $\{ \mathbf{w}_i \}_{i=1}^{I}$, their corresponding projections in the image $\{ \mathbf{x}_i \}_{i=1}^{I}$ and known intrinsic parameters $\boldsymbol{\Lambda}$. Goal - estimate the rotation and translation that map points in the coordinate system of the object to points in the coordinate system of the camera so that:</p>$$
\hat{\boldsymbol{\Omega}}, \hat{\boldsymbol{\tau}} = \argmax_{\boldsymbol{\Omega}, \boldsymbol{\tau}} \left[ \sum_{i=1}^{I}\log \left[ Pr(\mathbf{x}_i | \mathbf{w}_i, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}) \right]\right] \\
= \argmax_{\boldsymbol{\Omega}, \boldsymbol{\tau}} \left[ \sum_{i=1}^{I}\log \left[ \text{Norm}_{\mathbf{x}_i}\left[\text{pinhole}[\mathbf{w}_i, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}], \sigma^2\mathbf{I} \right] \right]\right]
$$<p>This is a maximum likelihood learning problem, where we aim to find parameters $\boldsymbol{\Omega}, \boldsymbol{\tau}$ that make the predictions $\text{pinhole}[\mathbf{w}, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}]$ of the model agree with the observed 2D points.</p><h3 id=problem-2-learning-intrinsic-parameters-calibration>Problem 2: Learning intrinsic parameters, calibration
<a class=heading-link href=#problem-2-learning-intrinsic-parameters-calibration><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This problem is known as <em>calibration</em>. The inputs are the same. Usually <em>the calibration target</em> is used to construct the points.</p>$$
\hat{\boldsymbol{\Lambda}} = \argmax_{\boldsymbol{\Lambda}}\left[ \max_{\boldsymbol{\Omega}, \boldsymbol{\tau}} \left[ \sum_{i=1}^{I}\log \left[ Pr(\mathbf{x}_i | \mathbf{w}_i, \boldsymbol{\Lambda}, \boldsymbol{\Omega}, \boldsymbol{\tau}) \right]\right] \right ].
$$<h3 id=problem-3-inferring-3d-world-points-triangulation>Problem 3: Inferring 3D world points, triangulation
<a class=heading-link href=#problem-3-inferring-3d-world-points-triangulation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><strong>Problem</strong>: estimate the 3D position of a point $\mathbf{w}$ in the scene, given its projections $\{ \mathbf{x}_j\}_{j=1}^{J}$ in $J\geq 2$ calibrated cameras. When $J=2$, this is known as a <em>calibrated stereo reconstruction</em>. With $J>2$ calibrated cameras, it is known to as <em>multiview reconstruction</em>. If the process is repeated for many points, the result is a sparse 3D point cloud.</p><p>Formal description: given $J$ calibrated cameras in known positions, viewing the same 3D point $\mathbf{w}$ and knowing the corresponding 2D projections $\{ \mathbf{x}_j\}_{j=1}^{J}$ in the $J$ images establish the 3D position $\mathbf{w}$ in the world:</p>$$
\hat{\mathbf{w}} = \argmax_{\mathbf{w}} \left[ \sum_{j=1}^{J}\log \left[ Pr(\mathbf{x}_j | \mathbf{w}, \boldsymbol{\Lambda}_j, \boldsymbol{\Omega}_j, \boldsymbol{\tau}_j) \right]\right].
$$<p>The principle behind reconstruction is known as <em>triangulation</em>.</p><h2 id=two-view-camera-geometry>Two-view camera geometry
<a class=heading-link href=#two-view-camera-geometry><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=key-terms-1>Key terms
<a class=heading-link href=#key-terms-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><img src=image.png alt="Alt text">
<strong>Epipolar line</strong> - the line in the image plane of one camera that corresponds to the projection of a 3D ray coming out of the optical center of another camera.<br><strong>Epipolar constraint</strong> - for any point in the first image, the corresponding point in the second image is constrained to lie on a line.
<img src=image-1.png alt="Alt text">
<strong>Epipole</strong> - a single point where all epipolar lines converge. It is the image in the second camera of the optical center of the first camera.</p><h3 id=the-essential-matrix>The essential matrix
<a class=heading-link href=#the-essential-matrix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>An arbitrary 3D point $\mathbf{w}$ is projected into the cameras as:</p>$$
\begin{split}
\lambda_1 \tilde{\mathbf{x}}_1 &= [\mathbf{I}, \mathbf{0}]\tilde{\mathbf{w}} \\
\lambda_2 \tilde{\mathbf{x}}_2 &= [\mathbf{\Omega}, \boldsymbol{\tau}]\tilde{\mathbf{w}}.
\end{split}
$$<p>Here it is assumed that the cameras are normalized ($\mathbf{\Lambda}_1 = \mathbf{\Lambda}_2 = \mathbf{I}$), $\lambda$ - arbitrary scale factors (any scalar multiple $\lambda$ represents the same 2D point).
Now, having the same point observed in the first and second camera, and both are expressed in homogeneous coordinate, we have:</p>$$
\begin{split}
\lambda_1 \tilde{\mathbf{x}}_1 &= \mathbf{w} \\
\lambda_2 \tilde{\mathbf{x}}_2 &= \mathbf{\Omega} \mathbf{w} +  \boldsymbol{\tau}.
\end{split}
$$<p>Substituting first into second:</p>$$
\lambda_2 \tilde{\mathbf{x}}_2 = \mathbf{\Omega} \lambda_1 \tilde{\mathbf{x}}_1 + \boldsymbol{\tau}.
$$<p>This is the expression that represents a constraint between the possible positions of the corresponding points in two images. It is only parametrized by the transform of the second camera relative to the first.
After some manipulations ([2], p. 360) we can get that:</p>$$
\tilde{\mathbf{x}}_2^T \mathbf{E} \tilde{\mathbf{x}}_1 = 0,
$$<p>where $\mathbf{E} = \boldsymbol{\tau}{\times}\mathbf{\Omega}$ is known as the essential matrix. This relation is the math constraint between the positions of corresponding points in two normalized cameras.</p><h3 id=the-fundamental-matrix>The fundamental matrix
<a class=heading-link href=#the-fundamental-matrix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>In the essential matrix it is assumed that the normalized camera models are used ($\boldsymbol{\Lambda}_1 = \boldsymbol{\Lambda}_2 = \mathbf{I}$).
The fundamental matrix is the same as the essential matrix for cameras with arbitrary intrinsics matrices $\boldsymbol{\Lambda}_1$ and $\boldsymbol{\Lambda}_2$.</p><p>The contraint imposed by the fundamental matrix has the following form:</p>$$
\tilde{\mathbf{x}}_2^T \mathbf{F} \tilde{\mathbf{x}}_1 = 0,
$$<p>where $\mathbf{F} = \boldsymbol{\Lambda}_2^{-T} \mathbf{E} \boldsymbol{\Lambda}_1^{-1}$ is the fundamental matrix, and $\tilde{\mathbf{x}}_1$ and $\tilde{\mathbf{x}}_2$ are the normalized image coordinates of the corresponding points in the two images.</p><p>As a result, the essential matrix can be recovered from the fundamental matrix by the following equation:</p>$$
\mathbf{E} = \boldsymbol{\Lambda}_2^T \mathbf{F} \boldsymbol{\Lambda}_1.
$$<h3 id=two-view-reconstruction-pipeline>Two-view reconstruction pipeline
<a class=heading-link href=#two-view-reconstruction-pipeline><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The rudimentary pipeline for 3D scene reconstruction consists of the following steps:</p><ol><li>Compute image features.</li><li>Compute feature descriptors.</li><li>Find initial matches:<ul><li>by computing the squared distance between the descriptors and checking that it exceeds a predefined threshold to filter out false matches;</li><li>by rejecting matches where the ratio between the quality of the best and second best match is close (identifying that alternative matches are plausible).</li></ul></li><li>Compute fundamental matrix.</li><li>Refine matches using the fundamental matrix and epipolar geometry.</li><li>Estimate essential matrix</li><li>Decompose essential matrix to obtain the relative camera pose.</li><li>Estimate 3D points.</li></ol><h3 id=rectification>Rectification
<a class=heading-link href=#rectification><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Process of aligning two images so that corresponding points lie on the same horizontal line in both images.</p></article></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></div><footer class=footer><section class=container>©
2024
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>