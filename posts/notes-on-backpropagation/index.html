<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Aleksandr Mikoff"><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on backpropagation"><meta name=twitter:description content="Notes on backpropagation In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky."><meta property="og:title" content="Notes on backpropagation"><meta property="og:description" content="Notes on backpropagation In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/notes-on-backpropagation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-19T23:00:00+03:00"><meta property="article:modified_time" content="2022-02-19T23:00:00+03:00"><base href=https://mikoff.github.io/posts/notes-on-backpropagation/><title>Notes on backpropagation ¬∑ Aleksandr Mikoff's blog</title><link rel=canonical href=https://mikoff.github.io/posts/notes-on-backpropagation/><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.11.2/css/all.css integrity=sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin=anonymous><link rel=stylesheet href=https://mikoff.github.io/fontdata/css/academicons.min.css><link rel=stylesheet href=https://mikoff.github.io/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/coder-dark.min.83a2010dac9f59f943b3004cd6c4f230507ad036da635d3621401d42ec4e2835.css integrity="sha256-g6IBDayfWflDswBM1sTyMFB60DbaY102IUAdQuxOKDU=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/image.css><link rel=stylesheet href=https://mikoff.github.io/css/spoiler.css><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-16x16.png sizes=16x16><meta name=generator content="Hugo 0.119.0"></head><body class=colorscheme-auto><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fas fa-bars"></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title>Notes on backpropagation</h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fas fa-calendar"></i>
<time datetime=2022-02-19T23:00:00+03:00>February 19, 2022</time></span>
<span class=reading-time><i class="fas fa-clock"></i>
10-minute read</span></div><div class=tags><i class="fas fa-tag"></i>
<a href=https://mikoff.github.io/tags/backpropagation/>Backpropagation</a>
<span class=separator>‚Ä¢</span>
<a href=https://mikoff.github.io/tags/automatic-differentiation/>Automatic differentiation</a>
<span class=separator>‚Ä¢</span>
<a href=https://mikoff.github.io/tags/deep-learning/>Deep learning</a></div></div></header><div><h1 id=notes-on-backpropagation>Notes on backpropagation</h1><p>In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.</p><p>However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky. The second - gradient calculations might be computationally demanding, especially if we have a large number of parameters: we have to find the gradient w.r.t. each of them. To ease the calculations and improve its efficiency the backpropagation algorithm can be used. From algorithmic point of view it allows us to:</p><ul><li>split the differentiation into the simple computational steps;</li><li>reuse the intermediate calculation results.</li></ul><h2 id=chain-rule-example>Chain rule: example</h2><p>Let&rsquo;s start from an example. We have the following function:
$$
f(x, \boldsymbol{w}, y) = \left(w_0 + w_1 x - y\right)^2
$$
to <em>minimize</em> it, we need to follow the negative of the gradient (since the gradient shows us the direction of growth).</p><p>To do so we have find its derivatives with respect to vector $\boldsymbol{w}$. For such a case it is possible to differentiate directly, but for educational purposes let&rsquo;s utilize the chain rule to compute the gradient of $f$ w.r.t. $\boldsymbol{w}$.
To do so we first split the original expression $f$ to the elementary operations:
$$
\begin{align}
&amp;a(w_1, x) &&= w_1 x ,&& D_{w_1}a = \frac{\partial a}{\partial w_1} = x & D_{x}a=\frac{\partial a}{\partial x} = w_1\\
&amp;b(a, w_0) &&= a + w_0 ,&& D_{a}b = \frac{\partial b}{\partial a} = 1 & D_{w_0}b=\frac{\partial b}{\partial w_0} = 1\\
&amp;c(b, y) &&= b - y ,&& D_{b}c = \frac{\partial c}{\partial b} = 1 & D_{y}c=\frac{\partial c}{\partial y} = -1\\
&amp;d(c) &&= c^2 ,&& D_{c}d = \frac{\partial d}{\partial c} = 2c
\end{align}
$$
and our target function can be written as the composition of elementary functions:
$$
f(x, \boldsymbol{w}, y) = d\left(c\left(b\left(a\left(w_1, x\right), w_0\right), y\right)\right).
$$
Their derivatives are known, and the <em>chaining</em> of the gradient expressions can be done using multiplication:
$$
\frac{\partial f}{\partial w_1} = \frac{\partial f}{\partial d} \frac{\partial d}{\partial c} \frac{\partial c}{\partial b} \frac{\partial b}{\partial a} \frac{\partial a}{\partial w_1},
$$
noting that $d$ and $f$ are the same functions the $\frac{\partial f}{\partial d} = 1$ and expanding:
$$
\frac{\partial f}{\partial w_1} = \frac{\partial d}{\partial c} \frac{\partial c}{\partial b} \frac{\partial b}{\partial a} \frac{\partial a}{\partial w_1} = 2 c x = 2 (b - y) x = 2 (w_0 + w_1x - y) x
$$</p><h2 id=computational-graph>Computational graph</h2><p>The visual representation of the computation sequence is shown on the following graph, where:</p><ul><li>‚û°Ô∏è the forward pass (üü¢ - green color) computes the output values from the inputs;</li><li>‚¨ÖÔ∏è the backward pass (üî¥ - red color) uses the chain rule to compute the gradients, on the diagram we show the numeric values of the negative of the gradient.</li></ul><p><img src=forward-backpropagation.drawio.svg#center alt=cg></p><p>After completion of the forward pass we have the inputs and outputs for all the nodes (shown in green). Now what we want to compute is the gradient that shows the sensitivity of parameters ${w_0, w_1}$ on target function $f$.
Since the local gradients for every node are also known we can compute the gradient w.r.t. the target function by simple multiplication.
For example, if we want to compute $\frac{\partial f}{\partial c}$ we note that we know the input $c = -3$, the derivative of $c^2$, and $\frac{\partial f}{\partial d} = 1$. Chaining the results we obtain:
$$
\frac{\partial f}{\partial c} = \frac{\partial f}{\partial d}\frac{\partial d}{\partial c} = 1 \cdot 2 \cdot c = 1 \cdot 2 \cdot 3 = -6.
$$
After performing this step we know how much the change in node $c$ affects the node $d$: that is the information, given by the derivative. The gradient shows the direction in which a function grows faster, but during the optimization we want to step towards its <strong>minimum</strong>. To do so we move in the direction of negative gradient. Let&rsquo;s look at numbers: what $d$ wants is to increase $c$ with a force of $6$ (we negate the numbers, because we are looking for minimum, right?). Does it make sense? Yes, our current value of $c$ is $-3$ and if it becomes higher, $d$ decreases.
If we continue this logic we note that $y$, for example, should decrease (since $-\frac{\partial f}{\partial y} = -6$) when other parameters are fixed: it also makes sense, because if $y$ increases it makes our target $f = (b - y)^2~|~b = 4$ higher and what we want to achieve is the opposite: we want to minimize $f$.</p><p>The nice and short explanation of the gradient flow process is given in <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><blockquote><p>To compute the gradient of some scalar $z$ with respect to one of its ancestors $x$ in the graph, we begin by observing that the gradient with respect to z is equal to 1.
We can compute the gradient with respect to each parent of $z$ in the graph by multiplying the current gradient by the Jacobian of the operation that produced $z$. We continue multiplying by Jacobians, traveling backward through the graph in this way until we reach $x$.</p></blockquote><p>The forward and backward passes for this case can be written in a few lines of code:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x, y <span style=color:#555>=</span> <span style=color:#f60>3</span>, <span style=color:#f60>7</span>
</span></span><span style=display:flex><span>w0, w1 <span style=color:#555>=</span> <span style=color:#555>-</span><span style=color:#f60>2</span>, <span style=color:#f60>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>a <span style=color:#555>=</span> x <span style=color:#555>*</span> w1
</span></span><span style=display:flex><span>b <span style=color:#555>=</span> a <span style=color:#555>+</span> w0
</span></span><span style=display:flex><span>c <span style=color:#555>=</span> b <span style=color:#555>-</span> y
</span></span><span style=display:flex><span>d <span style=color:#555>=</span> c <span style=color:#555>**</span> <span style=color:#f60>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># backward pass</span>
</span></span><span style=display:flex><span>df_dc <span style=color:#555>=</span> <span style=color:#f60>2</span> <span style=color:#555>*</span> c
</span></span><span style=display:flex><span>df_db <span style=color:#555>=</span> df_dc
</span></span><span style=display:flex><span>df_da <span style=color:#555>=</span> df_db
</span></span><span style=display:flex><span>df_dw1 <span style=color:#555>=</span> df_da <span style=color:#555>*</span> x
</span></span><span style=display:flex><span>df_dw0 <span style=color:#555>=</span> df_db
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;df/dw0 = </span><span style=color:#a00>{</span>df_dw0<span style=color:#a00>}</span><span style=color:#c30>, df/dw1 = </span><span style=color:#a00>{</span>df_dw1<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dw0 = -6, df/dw1 = -18
</code></pre><p>Note that even in such a simple case some intermediate results are reusable: we do not compute the results twice (for example, for $\frac{\partial f}{\partial c}$), we use the same <em>cached</em> value.</p><h2 id=patterns>Patterns</h2><p>According to <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, it is useful to keep in mind the interpretation of some operations on intuitive level:</p><ul><li><code>add</code> - takes the gradient and distributes it equally to all the inputs;</li><li><code>max</code> - routes the gradient to the largest value of inputs (only one);</li><li><code>multiply</code> - if we think about the multiplication as a constant term, then the gradients of the output are being split to the inputs inversely proportional to their values.</li></ul><h2 id=implementation>Implementation</h2><p>Thinking about the actual implementation we note that to calculate the final gradients we need to:</p><ul><li>correctly define the forward and backward walking order, it can be done using the graphs;</li><li>store the outputs of the nodes, because the local gradient depends on their values;</li><li>know the derivative of each node w.r.t. its inputs.</li></ul><p>First, let&rsquo;s define the nodes that perform the calculations, store the results and can calculate the gradient of the output w.r.t. the input:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>from</span> <span style=color:#0cf;font-weight:700>abc</span> <span style=color:#069;font-weight:700>import</span> ABC, abstractmethod
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Operation</span>(ABC):
</span></span><span style=display:flex><span>    <span style=color:#c30>&#34;&#34;&#34;Base class for node operations.
</span></span></span><span style=display:flex><span><span style=color:#c30>    
</span></span></span><span style=display:flex><span><span style=color:#c30>    On forward pass the inherited class calculates the output of the 
</span></span></span><span style=display:flex><span><span style=color:#c30>    node and nulls the local gradient for future calculations.
</span></span></span><span style=display:flex><span><span style=color:#c30>    
</span></span></span><span style=display:flex><span><span style=color:#c30>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self, inputs):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>inputs <span style=color:#555>=</span> inputs
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> <span style=color:#069;font-weight:700>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>local_grad <span style=color:#555>=</span> <span style=color:#f60>0.</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward</span>(self, <span style=color:#555>*</span>args):
</span></span><span style=display:flex><span>        <span style=color:#c30>&#34;&#34;&#34;Calculates the node output given inputs&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>local_grad <span style=color:#555>=</span> <span style=color:#f60>0.</span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>_forward(<span style=color:#555>*</span>args)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#99f>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, <span style=color:#555>*</span>inputs):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>pass</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#99f>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#c30>&#34;&#34;&#34;Returns the local gradient.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Multiply</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, x, y):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>x <span style=color:#555>=</span> x
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>y <span style=color:#555>=</span> y
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> x <span style=color:#555>*</span> y
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        dx <span style=color:#555>=</span> self<span style=color:#555>.</span>y <span style=color:#555>*</span> dz
</span></span><span style=display:flex><span>        dy <span style=color:#555>=</span> self<span style=color:#555>.</span>x <span style=color:#555>*</span> dz
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [dx, dy]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Add</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, x, y):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> x <span style=color:#555>+</span> y
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [dz, dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Sub</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, x, y):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> x <span style=color:#555>-</span> y
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [dz, <span style=color:#555>-</span>dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Square</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, x):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>x <span style=color:#555>=</span> x
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> x <span style=color:#555>*</span> x
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [<span style=color:#f60>2</span> <span style=color:#555>*</span> self<span style=color:#555>.</span>x <span style=color:#555>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Sigmoid</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>from</span> <span style=color:#0cf;font-weight:700>numpy</span> <span style=color:#069;font-weight:700>import</span> exp
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> <span style=color:#f60>1.</span> <span style=color:#555>/</span> (<span style=color:#f60>1.</span> <span style=color:#555>+</span> exp(<span style=color:#555>-</span>x))
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [(<span style=color:#f60>1</span> <span style=color:#555>-</span> self<span style=color:#555>.</span>output) <span style=color:#555>*</span> self<span style=color:#555>.</span>output <span style=color:#555>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Div</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, a, b):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>a <span style=color:#555>=</span> a
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>b <span style=color:#555>=</span> b
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> a <span style=color:#555>/</span> b
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [<span style=color:#f60>1</span> <span style=color:#555>/</span> self<span style=color:#555>.</span>b <span style=color:#555>*</span> dz, <span style=color:#555>-</span> self<span style=color:#555>.</span>a <span style=color:#555>/</span> self<span style=color:#555>.</span>b<span style=color:#555>**</span><span style=color:#f60>2</span> <span style=color:#555>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Inv</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_forward</span>(self, a):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>a <span style=color:#555>=</span> a
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> <span style=color:#f60>1</span> <span style=color:#555>/</span> a
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>output
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dz):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> [<span style=color:#555>-</span><span style=color:#f60>1.</span> <span style=color:#555>/</span> self<span style=color:#555>.</span>a<span style=color:#555>**</span><span style=color:#f60>2</span> <span style=color:#555>*</span> dz]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>Input</span>(<span style=color:#366>object</span>):
</span></span><span style=display:flex><span>    <span style=color:#c30>&#34;&#34;&#34;Class for input values&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self, value):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output <span style=color:#555>=</span> value
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>local_grad <span style=color:#555>=</span> <span style=color:#f60>0.0</span>
</span></span></code></pre></div></div></div><p>Now let&rsquo;s define the <code>ComputationalGraph</code> class, that is responsible for chaining and calculating the forward and backward pass for the graph:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>ComputationalGraph</span>:
</span></span><span style=display:flex><span>    <span style=color:#c30>&#34;&#34;&#34;Computational graph: stores computing nodes, gradients and inputs.
</span></span></span><span style=display:flex><span><span style=color:#c30>    
</span></span></span><span style=display:flex><span><span style=color:#c30>    The computational graph is used to calculate the output of each node
</span></span></span><span style=display:flex><span><span style=color:#c30>    on forward pass and to calculate the local gradients of each node with
</span></span></span><span style=display:flex><span><span style=color:#c30>    respect to the node inputs.
</span></span></span><span style=display:flex><span><span style=color:#c30>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>operations <span style=color:#555>=</span> {}
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>inputs <span style=color:#555>=</span> {}
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>nodes <span style=color:#555>=</span> {}
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>add_inputs</span>(self, <span style=color:#555>**</span>kwargs):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> name, value <span style=color:#000;font-weight:700>in</span> kwargs<span style=color:#555>.</span>items():
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>inputs[name] <span style=color:#555>=</span> Input(value)
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>nodes[name] <span style=color:#555>=</span> self<span style=color:#555>.</span>inputs[name]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>add_computation_node</span>(self, name, operation, inputs):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>operations[name] <span style=color:#555>=</span> operation(inputs)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>nodes[name] <span style=color:#555>=</span> self<span style=color:#555>.</span>operations[name]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>_get_traverse_order</span>(self, root):
</span></span><span style=display:flex><span>        route <span style=color:#555>=</span> []
</span></span><span style=display:flex><span>        queue <span style=color:#555>=</span> [root]
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>while</span> <span style=color:#366>len</span>(queue):
</span></span><span style=display:flex><span>            prev <span style=color:#555>=</span> queue<span style=color:#555>.</span>pop(<span style=color:#f60>0</span>)
</span></span><span style=display:flex><span>            <span style=color:#069;font-weight:700>if</span> prev <span style=color:#000;font-weight:700>not</span> <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>inputs:
</span></span><span style=display:flex><span>                route<span style=color:#555>.</span>append(prev)
</span></span><span style=display:flex><span>            <span style=color:#069;font-weight:700>if</span> prev <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>operations:
</span></span><span style=display:flex><span>                queue<span style=color:#555>.</span>extend(self<span style=color:#555>.</span>operations[prev]<span style=color:#555>.</span>inputs)
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> route
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward</span>(self, root):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> name <span style=color:#000;font-weight:700>in</span> <span style=color:#366>reversed</span>(self<span style=color:#555>.</span>_get_traverse_order(root)):
</span></span><span style=display:flex><span>            node <span style=color:#555>=</span> self<span style=color:#555>.</span>nodes[name]
</span></span><span style=display:flex><span>            node<span style=color:#555>.</span>forward(<span style=color:#555>*</span>[self<span style=color:#555>.</span>nodes[i]<span style=color:#555>.</span>output <span style=color:#069;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> node<span style=color:#555>.</span>inputs])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> inp <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>inputs:
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>inputs[inp]<span style=color:#555>.</span>local_grad <span style=color:#555>=</span> <span style=color:#f60>0.0</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, root):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>operations[root]<span style=color:#555>.</span>local_grad <span style=color:#555>=</span> <span style=color:#f60>1.</span>
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> name <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>_get_traverse_order(root):
</span></span><span style=display:flex><span>            local_grad <span style=color:#555>=</span> self<span style=color:#555>.</span>nodes[name]<span style=color:#555>.</span>local_grad
</span></span><span style=display:flex><span>            inputs <span style=color:#555>=</span> self<span style=color:#555>.</span>nodes[name]<span style=color:#555>.</span>inputs
</span></span><span style=display:flex><span>            gradients_wrt_input <span style=color:#555>=</span> self<span style=color:#555>.</span>nodes[name]<span style=color:#555>.</span>backward(local_grad)
</span></span><span style=display:flex><span>            <span style=color:#09f;font-style:italic># roll back to inputs</span>
</span></span><span style=display:flex><span>            <span style=color:#069;font-weight:700>for</span> parent, g <span style=color:#000;font-weight:700>in</span> <span style=color:#366>zip</span>(inputs, gradients_wrt_input):
</span></span><span style=display:flex><span>                self<span style=color:#555>.</span>nodes[parent]<span style=color:#555>.</span>local_grad <span style=color:#555>+=</span> g
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>get_gradient</span>(self, node):
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>if</span> node <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>nodes:
</span></span><span style=display:flex><span>            <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>nodes[node]<span style=color:#555>.</span>local_grad
</span></span></code></pre></div></div></div><p>Now we can make the real job, let&rsquo;s calculate the gradient of the following function w.r.t. its inputs, $x$ and $y$:
$$
f(x, y) = \frac{x + \sigma(y)}{\sigma(x) + (x + y)^2}.
$$</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#555>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_inputs(x <span style=color:#555>=</span> <span style=color:#f60>2.</span>, y <span style=color:#555>=</span> <span style=color:#555>-</span><span style=color:#f60>4.</span>)
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sigy&#39;</span>, Sigmoid, (<span style=color:#c30>&#39;y&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;num&#39;</span>, Add, (<span style=color:#c30>&#39;x&#39;</span>, <span style=color:#c30>&#39;sigy&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sigx&#39;</span>, Sigmoid, (<span style=color:#c30>&#39;x&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;xpy&#39;</span>, Add, (<span style=color:#c30>&#39;x&#39;</span>, <span style=color:#c30>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sq&#39;</span>, Square, (<span style=color:#c30>&#39;xpy&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;denom&#39;</span>, Add, (<span style=color:#c30>&#39;sigx&#39;</span>, <span style=color:#c30>&#39;sq&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;f&#39;</span>, Div, (<span style=color:#c30>&#39;num&#39;</span>, <span style=color:#c30>&#39;denom&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>forward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>backward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dx <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>dy <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;y&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;df/dx = </span><span style=color:#a00>{</span>dx<span style=color:#a00>}</span><span style=color:#c30>, df/dy = </span><span style=color:#a00>{</span>dy<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dx = 0.5348320870757595, df/dy = 0.342460382923052
</code></pre><p>And verify the result on an additional example that we studied in the beginning:
$$
f(x, \boldsymbol{w}, y) = \left(w_0 + w_1 x - y\right)^2.
$$</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#555>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_inputs(x <span style=color:#555>=</span> <span style=color:#f60>3.</span>, y <span style=color:#555>=</span> <span style=color:#f60>7.</span>, w0 <span style=color:#555>=</span> <span style=color:#555>-</span><span style=color:#f60>2</span>, w1 <span style=color:#555>=</span> <span style=color:#f60>2</span>)
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;w1_times_x&#39;</span>, Multiply, (<span style=color:#c30>&#39;w1&#39;</span>, <span style=color:#c30>&#39;x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;prediction&#39;</span>, Add, (<span style=color:#c30>&#39;w0&#39;</span>, <span style=color:#c30>&#39;w1_times_x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;l&#39;</span>, Sub, (<span style=color:#c30>&#39;prediction&#39;</span>, <span style=color:#c30>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;f&#39;</span>, Square, (<span style=color:#c30>&#39;l&#39;</span>,))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>forward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>backward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dw0 <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;w0&#39;</span>)
</span></span><span style=display:flex><span>dw1 <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;w1&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;df/dw0 = </span><span style=color:#a00>{</span>dw0<span style=color:#a00>}</span><span style=color:#c30>, df/dw1 = </span><span style=color:#a00>{</span>dw1<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dw0 = -6.0, df/dw1 = -18.0
</code></pre><p>Result is exactly the same.
As a final step let&rsquo;s use this scheme to find the minimum of the following function:
$$
f(x, y) = x^2 + y^2 + 400x + 8y
$$</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#555>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_inputs(x <span style=color:#555>=</span> <span style=color:#f60>0.</span>, y <span style=color:#555>=</span> <span style=color:#f60>0.</span>, a <span style=color:#555>=</span> <span style=color:#f60>400.</span>, b <span style=color:#555>=</span> <span style=color:#f60>8.</span>)
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sqx&#39;</span>, Square, (<span style=color:#c30>&#39;x&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sqy&#39;</span>, Square, (<span style=color:#c30>&#39;y&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;ax&#39;</span>, Multiply, (<span style=color:#c30>&#39;a&#39;</span>, <span style=color:#c30>&#39;x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;by&#39;</span>, Multiply, (<span style=color:#c30>&#39;b&#39;</span>, <span style=color:#c30>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;sqxpsqy&#39;</span>, Add, (<span style=color:#c30>&#39;sqx&#39;</span>, <span style=color:#c30>&#39;sqy&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;axpby&#39;</span>, Add, (<span style=color:#c30>&#39;ax&#39;</span>, <span style=color:#c30>&#39;by&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#555>.</span>add_computation_node(<span style=color:#c30>&#39;f&#39;</span>, Add, (<span style=color:#c30>&#39;sqxpsqy&#39;</span>, <span style=color:#c30>&#39;axpby&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#366>range</span>(<span style=color:#f60>0</span>, <span style=color:#f60>50</span>):
</span></span><span style=display:flex><span>    cg<span style=color:#555>.</span>forward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>    cg<span style=color:#555>.</span>backward(<span style=color:#c30>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dx <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>    dy <span style=color:#555>=</span> cg<span style=color:#555>.</span>get_gradient(<span style=color:#c30>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>    cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;x&#39;</span>]<span style=color:#555>.</span>output <span style=color:#555>-=</span> dx <span style=color:#555>*</span> <span style=color:#f60>0.1</span>
</span></span><span style=display:flex><span>    cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;y&#39;</span>]<span style=color:#555>.</span>output <span style=color:#555>-=</span> dy <span style=color:#555>*</span> <span style=color:#f60>0.1</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>&#39;Minimum:&#39;</span>, cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;x&#39;</span>]<span style=color:#555>.</span>output, cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;y&#39;</span>]<span style=color:#555>.</span>output)
</span></span></code></pre></div><pre><code>Minimum: -199.9971455046146 -3.9999429100922916
</code></pre><p>All done! Minimum was correctly found, it is time to visual assess it.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>import</span> <span style=color:#0cf;font-weight:700>matplotlib.pylab</span> <span style=color:#069;font-weight:700>as</span> <span style=color:#0cf;font-weight:700>plt</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>import</span> <span style=color:#0cf;font-weight:700>numpy</span> <span style=color:#069;font-weight:700>as</span> <span style=color:#0cf;font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=color:#555>%</span>matplotlib inline
</span></span><span style=display:flex><span><span style=color:#555>%</span>config InlineBackend<span style=color:#555>.</span>figure_format<span style=color:#555>=</span><span style=color:#c30>&#39;retina&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x, y <span style=color:#555>=</span> np<span style=color:#555>.</span>meshgrid(np<span style=color:#555>.</span>arange(<span style=color:#555>-</span><span style=color:#f60>800</span>, <span style=color:#f60>800</span>), np<span style=color:#555>.</span>arange(<span style=color:#555>-</span><span style=color:#f60>400</span>, <span style=color:#f60>400</span>))
</span></span><span style=display:flex><span>z <span style=color:#555>=</span> x<span style=color:#555>**</span><span style=color:#f60>2</span> <span style=color:#555>+</span> y<span style=color:#555>**</span><span style=color:#f60>2</span> <span style=color:#555>+</span> <span style=color:#f60>400</span><span style=color:#555>*</span>x <span style=color:#555>+</span> <span style=color:#f60>8</span><span style=color:#555>*</span>y 
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>3</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>imshow(z, extent<span style=color:#555>=</span>[x<span style=color:#555>.</span>min(), x<span style=color:#555>.</span>max(), y<span style=color:#555>.</span>min(), y<span style=color:#555>.</span>max()], origin<span style=color:#555>=</span><span style=color:#c30>&#34;lower&#34;</span>, cmap<span style=color:#555>=</span><span style=color:#c30>&#39;cool&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter([cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;x&#39;</span>]<span style=color:#555>.</span>output], [cg<span style=color:#555>.</span>inputs[<span style=color:#c30>&#39;y&#39;</span>]<span style=color:#555>.</span>output], c <span style=color:#555>=</span> <span style=color:#c30>&#39;red&#39;</span>, label<span style=color:#555>=</span><span style=color:#c30>&#39;Min&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel<span style=color:#555>=</span><span style=color:#c30>&#39;$x$&#39;</span>, ylabel<span style=color:#555>=</span><span style=color:#c30>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend()
</span></span></code></pre></div></div></div><p><img src=output_23_1.png#center alt=png></p><p>The code is available here:
<a href=https://github.com/mikoff/blog_projects/tree/master/auto_differentation>https://github.com/mikoff/blog_projects/tree/master/auto_differentation</a></p><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.deeplearningbook.org/>https://www.deeplearningbook.org/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://cs231n.github.io/optimization-2/>https://cs231n.github.io/optimization-2/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>¬© 2023
Aleksandr Mikoff
¬∑
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.slim.min.js></script></main></body></html>