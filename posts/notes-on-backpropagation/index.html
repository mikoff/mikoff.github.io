<!doctype html><html lang=en><head><title>Notes on backpropagation ¬∑ Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Notes on backpropagation Link to heading In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on backpropagation"><meta name=twitter:description content="Notes on backpropagation Link to heading In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky."><meta property="og:title" content="Notes on backpropagation"><meta property="og:description" content="Notes on backpropagation Link to heading In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/notes-on-backpropagation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-19T23:00:00+03:00"><meta property="article:modified_time" content="2022-02-19T23:00:00+03:00"><link rel=canonical href=https://mikoff.github.io/posts/notes-on-backpropagation/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://mikoff.github.io/posts/notes-on-backpropagation/>Notes on backpropagation</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2022-02-19T23:00:00+03:00>February 19, 2022
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
10-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/backpropagation/>Backpropagation</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/automatic-differentiation/>Automatic Differentiation</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/deep-learning/>Deep Learning</a></span></div></div></header><div class=post-content><h1 id=notes-on-backpropagation>Notes on backpropagation
<a class=heading-link href=#notes-on-backpropagation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.</p><p>However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky. The second - gradient calculations might be computationally demanding, especially if we have a large number of parameters: we have to find the gradient w.r.t. each of them. To ease the calculations and improve its efficiency the backpropagation algorithm can be used. From algorithmic point of view it allows us to:</p><ul><li>split the differentiation into the simple computational steps;</li><li>reuse the intermediate calculation results.</li></ul><h2 id=chain-rule-example>Chain rule: example
<a class=heading-link href=#chain-rule-example><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Let&rsquo;s start from an example. We have the following function:</p>$$
f(x, \boldsymbol{w}, y) = \left(w_0 + w_1 x - y\right)^2
$$<p>to <em>minimize</em> it, we need to follow the negative of the gradient (since the gradient shows us the direction of growth).</p><p>To do so we have find its derivatives with respect to vector $\boldsymbol{w}$. For such a case it is possible to differentiate directly, but for educational purposes let&rsquo;s utilize the chain rule to compute the gradient of $f$ w.r.t. $\boldsymbol{w}$.
To do so we first split the original expression $f$ to the elementary operations:</p>$$
\begin{align}
&a(w_1, x) &&= w_1 x ,&& D_{w_1}a = \frac{\partial a}{\partial w_1} = x & D_{x}a=\frac{\partial a}{\partial x} = w_1\\\\
&b(a, w_0) &&= a + w_0 ,&& D_{a}b = \frac{\partial b}{\partial a} = 1 & D_{w_0}b=\frac{\partial b}{\partial w_0} = 1\\\\
&c(b, y) &&= b - y ,&& D_{b}c = \frac{\partial c}{\partial b} = 1 & D_{y}c=\frac{\partial c}{\partial y} = -1\\\\
&d(c) &&= c^2 ,&& D_{c}d = \frac{\partial d}{\partial c} = 2c
\end{align}
$$<p>and our target function can be written as the composition of elementary functions:</p>$$
f(x, \boldsymbol{w}, y) = d\left(c\left(b\left(a\left(w_1, x\right), w_0\right), y\right)\right).
$$<p>Their derivatives are known, and the <em>chaining</em> of the gradient expressions can be done using multiplication:</p>$$
\frac{\partial f}{\partial w_1} = \frac{\partial f}{\partial d} \frac{\partial d}{\partial c} \frac{\partial c}{\partial b} \frac{\partial b}{\partial a} \frac{\partial a}{\partial w_1},
$$<p>noting that $d$ and $f$ are the same functions the $\frac{\partial f}{\partial d} = 1$ and expanding:</p>$$
\frac{\partial f}{\partial w_1} = \frac{\partial d}{\partial c} \frac{\partial c}{\partial b} \frac{\partial b}{\partial a} \frac{\partial a}{\partial w_1} = 2 c x = 2 (b - y) x = 2 (w_0 + w_1x - y) x
$$<h2 id=computational-graph>Computational graph
<a class=heading-link href=#computational-graph><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The visual representation of the computation sequence is shown on the following graph, where:</p><ul><li>‚û°Ô∏è the forward pass (üü¢ - green color) computes the output values from the inputs;</li><li>‚¨ÖÔ∏è the backward pass (üî¥ - red color) uses the chain rule to compute the gradients, on the diagram we show the numeric values of the negative of the gradient.</li></ul><p><img src=forward-backpropagation.drawio.svg#center alt=cg></p><p>After completion of the forward pass we have the inputs and outputs for all the nodes (shown in green). Now what we want to compute is the gradient that shows the sensitivity of parameters $\{w_0, w_1\}$ on target function $f$.
Since the local gradients for every node are also known we can compute the gradient w.r.t. the target function by simple multiplication.
For example, if we want to compute $\frac{\partial f}{\partial c}$ we note that we know the input $c = -3$, the derivative of $c^2$, and $\frac{\partial f}{\partial d} = 1$. Chaining the results we obtain:</p>$$
\frac{\partial f}{\partial c} = \frac{\partial f}{\partial d}\frac{\partial d}{\partial c} = 1 \cdot 2 \cdot c = 1 \cdot 2 \cdot 3 = -6.
$$<p>After performing this step we know how much the change in node $c$ affects the node $d$: that is the information, given by the derivative. The gradient shows the direction in which a function grows faster, but during the optimization we want to step towards its <strong>minimum</strong>. To do so we move in the direction of negative gradient. Let&rsquo;s look at numbers: what $d$ wants is to increase $c$ with a force of $6$ (we negate the numbers, because we are looking for minimum, right?). Does it make sense? Yes, our current value of $c$ is $-3$ and if it becomes higher, $d$ decreases.
If we continue this logic we note that $y$, for example, should decrease (since $-\frac{\partial f}{\partial y} = -6$) when other parameters are fixed: it also makes sense, because if $y$ increases it makes our target $f = (b - y)^2~|~b = 4$ higher and what we want to achieve is the opposite: we want to minimize $f$.</p><p>The nice and short explanation of the gradient flow process is given in <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><blockquote><p>To compute the gradient of some scalar $z$ with respect to one of its ancestors $x$ in the graph, we begin by observing that the gradient with respect to z is equal to 1.
We can compute the gradient with respect to each parent of $z$ in the graph by multiplying the current gradient by the Jacobian of the operation that produced $z$. We continue multiplying by Jacobians, traveling backward through the graph in this way until we reach $x$.</p></blockquote><p>The forward and backward passes for this case can be written in a few lines of code:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x, y <span style=color:#000;font-weight:700>=</span> <span style=color:#099>3</span>, <span style=color:#099>7</span>
</span></span><span style=display:flex><span>w0, w1 <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span><span style=color:#099>2</span>, <span style=color:#099>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># forward pass</span>
</span></span><span style=display:flex><span>a <span style=color:#000;font-weight:700>=</span> x <span style=color:#000;font-weight:700>*</span> w1
</span></span><span style=display:flex><span>b <span style=color:#000;font-weight:700>=</span> a <span style=color:#000;font-weight:700>+</span> w0
</span></span><span style=display:flex><span>c <span style=color:#000;font-weight:700>=</span> b <span style=color:#000;font-weight:700>-</span> y
</span></span><span style=display:flex><span>d <span style=color:#000;font-weight:700>=</span> c <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># backward pass</span>
</span></span><span style=display:flex><span>df_dc <span style=color:#000;font-weight:700>=</span> <span style=color:#099>2</span> <span style=color:#000;font-weight:700>*</span> c
</span></span><span style=display:flex><span>df_db <span style=color:#000;font-weight:700>=</span> df_dc
</span></span><span style=display:flex><span>df_da <span style=color:#000;font-weight:700>=</span> df_db
</span></span><span style=display:flex><span>df_dw1 <span style=color:#000;font-weight:700>=</span> df_da <span style=color:#000;font-weight:700>*</span> x
</span></span><span style=display:flex><span>df_dw0 <span style=color:#000;font-weight:700>=</span> df_db
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;df/dw0 = </span><span style=color:#d14>{</span>df_dw0<span style=color:#d14>}</span><span style=color:#d14>, df/dw1 = </span><span style=color:#d14>{</span>df_dw1<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dw0 = -6, df/dw1 = -18
</code></pre><p>Note that even in such a simple case some intermediate results are reusable: we do not compute the results twice (for example, for $\frac{\partial f}{\partial c}$), we use the same <em>cached</em> value.</p><h2 id=patterns>Patterns
<a class=heading-link href=#patterns><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>According to <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, it is useful to keep in mind the interpretation of some operations on intuitive level:</p><ul><li><code>add</code> - takes the gradient and distributes it equally to all the inputs;</li><li><code>max</code> - routes the gradient to the largest value of inputs (only one);</li><li><code>multiply</code> - if we think about the multiplication as a constant term, then the gradients of the output are being split to the inputs inversely proportional to their values.</li></ul><h2 id=implementation>Implementation
<a class=heading-link href=#implementation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Thinking about the actual implementation we note that to calculate the final gradients we need to:</p><ul><li>correctly define the forward and backward walking order, it can be done using the graphs;</li><li>store the outputs of the nodes, because the local gradient depends on their values;</li><li>know the derivative of each node w.r.t. its inputs.</li></ul><p>First, let&rsquo;s define the nodes that perform the calculations, store the results and can calculate the gradient of the output w.r.t. the input:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>abc</span> <span style=color:#000;font-weight:700>import</span> ABC, abstractmethod
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Operation</span>(ABC):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;Base class for node operations.
</span></span></span><span style=display:flex><span><span style=color:#d14>    
</span></span></span><span style=display:flex><span><span style=color:#d14>    On forward pass the inherited class calculates the output of the 
</span></span></span><span style=display:flex><span><span style=color:#d14>    node and nulls the local gradient for future calculations.
</span></span></span><span style=display:flex><span><span style=color:#d14>    
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> __init__(<span style=color:#999>self</span>, inputs):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs <span style=color:#000;font-weight:700>=</span> inputs
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>None</span>
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>forward</span>(<span style=color:#999>self</span>, <span style=color:#000;font-weight:700>*</span>args):
</span></span><span style=display:flex><span>        <span style=color:#d14>&#34;&#34;&#34;Calculates the node output given inputs&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.</span>
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>_forward(<span style=color:#000;font-weight:700>*</span>args)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#3c5d5d;font-weight:700>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, <span style=color:#000;font-weight:700>*</span>inputs):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>pass</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#3c5d5d;font-weight:700>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#d14>&#34;&#34;&#34;Returns the local gradient.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Multiply</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, x, y):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>x <span style=color:#000;font-weight:700>=</span> x
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>y <span style=color:#000;font-weight:700>=</span> y
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> x <span style=color:#000;font-weight:700>*</span> y
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        dx <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>y <span style=color:#000;font-weight:700>*</span> dz
</span></span><span style=display:flex><span>        dy <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>x <span style=color:#000;font-weight:700>*</span> dz
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [dx, dy]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Add</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, x, y):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> x <span style=color:#000;font-weight:700>+</span> y
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [dz, dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Sub</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, x, y):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> x <span style=color:#000;font-weight:700>-</span> y
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [dz, <span style=color:#000;font-weight:700>-</span>dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Square</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, x):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>x <span style=color:#000;font-weight:700>=</span> x
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> x <span style=color:#000;font-weight:700>*</span> x
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [<span style=color:#099>2</span> <span style=color:#000;font-weight:700>*</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>x <span style=color:#000;font-weight:700>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Sigmoid</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, x):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>from</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>import</span> exp
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1.</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>1.</span> <span style=color:#000;font-weight:700>+</span> exp(<span style=color:#000;font-weight:700>-</span>x))
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [(<span style=color:#099>1</span> <span style=color:#000;font-weight:700>-</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output) <span style=color:#000;font-weight:700>*</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Div</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, a, b):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>a <span style=color:#000;font-weight:700>=</span> a
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>b <span style=color:#000;font-weight:700>=</span> b
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> a <span style=color:#000;font-weight:700>/</span> b
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [<span style=color:#099>1</span> <span style=color:#000;font-weight:700>/</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>b <span style=color:#000;font-weight:700>*</span> dz, <span style=color:#000;font-weight:700>-</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>a <span style=color:#000;font-weight:700>/</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>b<span style=color:#000;font-weight:700>**</span><span style=color:#099>2</span> <span style=color:#000;font-weight:700>*</span> dz]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Inv</span>(Operation):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_forward</span>(<span style=color:#999>self</span>, a):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>a <span style=color:#000;font-weight:700>=</span> a
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1</span> <span style=color:#000;font-weight:700>/</span> a
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, dz):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> [<span style=color:#000;font-weight:700>-</span><span style=color:#099>1.</span> <span style=color:#000;font-weight:700>/</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>a<span style=color:#000;font-weight:700>**</span><span style=color:#099>2</span> <span style=color:#000;font-weight:700>*</span> dz]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>Input</span>(<span style=color:#0086b3>object</span>):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;Class for input values&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> __init__(<span style=color:#999>self</span>, value):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>=</span> value
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.0</span>
</span></span></code></pre></div></div></div><p>Now let&rsquo;s define the <code>ComputationalGraph</code> class, that is responsible for chaining and calculating the forward and backward pass for the graph:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>ComputationalGraph</span>:
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;Computational graph: stores computing nodes, gradients and inputs.
</span></span></span><span style=display:flex><span><span style=color:#d14>    
</span></span></span><span style=display:flex><span><span style=color:#d14>    The computational graph is used to calculate the output of each node
</span></span></span><span style=display:flex><span><span style=color:#d14>    on forward pass and to calculate the local gradients of each node with
</span></span></span><span style=display:flex><span><span style=color:#d14>    respect to the node inputs.
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> __init__(<span style=color:#999>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations <span style=color:#000;font-weight:700>=</span> {}
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs <span style=color:#000;font-weight:700>=</span> {}
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes <span style=color:#000;font-weight:700>=</span> {}
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>add_inputs</span>(<span style=color:#999>self</span>, <span style=color:#000;font-weight:700>**</span>kwargs):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>for</span> name, value <span style=color:#000;font-weight:700>in</span> kwargs<span style=color:#000;font-weight:700>.</span>items():
</span></span><span style=display:flex><span>            <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs[name] <span style=color:#000;font-weight:700>=</span> Input(value)
</span></span><span style=display:flex><span>            <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name] <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs[name]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>add_computation_node</span>(<span style=color:#999>self</span>, name, operation, inputs):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations[name] <span style=color:#000;font-weight:700>=</span> operation(inputs)
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name] <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations[name]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>_get_traverse_order</span>(<span style=color:#999>self</span>, root):
</span></span><span style=display:flex><span>        route <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>        queue <span style=color:#000;font-weight:700>=</span> [root]
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>while</span> <span style=color:#0086b3>len</span>(queue):
</span></span><span style=display:flex><span>            prev <span style=color:#000;font-weight:700>=</span> queue<span style=color:#000;font-weight:700>.</span>pop(<span style=color:#099>0</span>)
</span></span><span style=display:flex><span>            <span style=color:#000;font-weight:700>if</span> prev <span style=color:#000;font-weight:700>not</span> <span style=color:#000;font-weight:700>in</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs:
</span></span><span style=display:flex><span>                route<span style=color:#000;font-weight:700>.</span>append(prev)
</span></span><span style=display:flex><span>            <span style=color:#000;font-weight:700>if</span> prev <span style=color:#000;font-weight:700>in</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations:
</span></span><span style=display:flex><span>                queue<span style=color:#000;font-weight:700>.</span>extend(<span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations[prev]<span style=color:#000;font-weight:700>.</span>inputs)
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>return</span> route
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>forward</span>(<span style=color:#999>self</span>, root):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>for</span> name <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>reversed</span>(<span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>_get_traverse_order(root)):
</span></span><span style=display:flex><span>            node <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name]
</span></span><span style=display:flex><span>            node<span style=color:#000;font-weight:700>.</span>forward(<span style=color:#000;font-weight:700>*</span>[<span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[i]<span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> node<span style=color:#000;font-weight:700>.</span>inputs])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>for</span> inp <span style=color:#000;font-weight:700>in</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs:
</span></span><span style=display:flex><span>            <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>inputs[inp]<span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.0</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>backward</span>(<span style=color:#999>self</span>, root):
</span></span><span style=display:flex><span>        <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>operations[root]<span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1.</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>for</span> name <span style=color:#000;font-weight:700>in</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>_get_traverse_order(root):
</span></span><span style=display:flex><span>            local_grad <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name]<span style=color:#000;font-weight:700>.</span>local_grad
</span></span><span style=display:flex><span>            inputs <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name]<span style=color:#000;font-weight:700>.</span>inputs
</span></span><span style=display:flex><span>            gradients_wrt_input <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[name]<span style=color:#000;font-weight:700>.</span>backward(local_grad)
</span></span><span style=display:flex><span>            <span style=color:#998;font-style:italic># roll back to inputs</span>
</span></span><span style=display:flex><span>            <span style=color:#000;font-weight:700>for</span> parent, g <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>zip</span>(inputs, gradients_wrt_input):
</span></span><span style=display:flex><span>                <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[parent]<span style=color:#000;font-weight:700>.</span>local_grad <span style=color:#000;font-weight:700>+=</span> g
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>get_gradient</span>(<span style=color:#999>self</span>, node):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>if</span> node <span style=color:#000;font-weight:700>in</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes:
</span></span><span style=display:flex><span>            <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>nodes[node]<span style=color:#000;font-weight:700>.</span>local_grad
</span></span></code></pre></div></div></div><p>Now we can make the real job, let&rsquo;s calculate the gradient of the following function w.r.t. its inputs, $x$ and $y$:</p>$$
f(x, y) = \frac{x + \sigma(y)}{\sigma(x) + (x + y)^2}.
$$<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#000;font-weight:700>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_inputs(x <span style=color:#000;font-weight:700>=</span> <span style=color:#099>2.</span>, y <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span><span style=color:#099>4.</span>)
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sigy&#39;</span>, Sigmoid, (<span style=color:#d14>&#39;y&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;num&#39;</span>, Add, (<span style=color:#d14>&#39;x&#39;</span>, <span style=color:#d14>&#39;sigy&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sigx&#39;</span>, Sigmoid, (<span style=color:#d14>&#39;x&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;xpy&#39;</span>, Add, (<span style=color:#d14>&#39;x&#39;</span>, <span style=color:#d14>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sq&#39;</span>, Square, (<span style=color:#d14>&#39;xpy&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;denom&#39;</span>, Add, (<span style=color:#d14>&#39;sigx&#39;</span>, <span style=color:#d14>&#39;sq&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;f&#39;</span>, Div, (<span style=color:#d14>&#39;num&#39;</span>, <span style=color:#d14>&#39;denom&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>forward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>backward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dx <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>dy <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;y&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;df/dx = </span><span style=color:#d14>{</span>dx<span style=color:#d14>}</span><span style=color:#d14>, df/dy = </span><span style=color:#d14>{</span>dy<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dx = 0.5348320870757595, df/dy = 0.342460382923052
</code></pre><p>And verify the result on an additional example that we studied in the beginning:</p>$$
f(x, \boldsymbol{w}, y) = \left(w_0 + w_1 x - y\right)^2.
$$<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#000;font-weight:700>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_inputs(x <span style=color:#000;font-weight:700>=</span> <span style=color:#099>3.</span>, y <span style=color:#000;font-weight:700>=</span> <span style=color:#099>7.</span>, w0 <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>-</span><span style=color:#099>2</span>, w1 <span style=color:#000;font-weight:700>=</span> <span style=color:#099>2</span>)
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;w1_times_x&#39;</span>, Multiply, (<span style=color:#d14>&#39;w1&#39;</span>, <span style=color:#d14>&#39;x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;prediction&#39;</span>, Add, (<span style=color:#d14>&#39;w0&#39;</span>, <span style=color:#d14>&#39;w1_times_x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;l&#39;</span>, Sub, (<span style=color:#d14>&#39;prediction&#39;</span>, <span style=color:#d14>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;f&#39;</span>, Square, (<span style=color:#d14>&#39;l&#39;</span>,))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>forward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>backward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dw0 <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;w0&#39;</span>)
</span></span><span style=display:flex><span>dw1 <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;w1&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;df/dw0 = </span><span style=color:#d14>{</span>dw0<span style=color:#d14>}</span><span style=color:#d14>, df/dw1 = </span><span style=color:#d14>{</span>dw1<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div><pre><code>df/dw0 = -6.0, df/dw1 = -18.0
</code></pre><p>Result is exactly the same.
As a final step let&rsquo;s use this scheme to find the minimum of the following function:</p>$$
f(x, y) = x^2 + y^2 + 400x + 8y
$$<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cg <span style=color:#000;font-weight:700>=</span> ComputationalGraph()
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_inputs(x <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.</span>, y <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.</span>, a <span style=color:#000;font-weight:700>=</span> <span style=color:#099>400.</span>, b <span style=color:#000;font-weight:700>=</span> <span style=color:#099>8.</span>)
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sqx&#39;</span>, Square, (<span style=color:#d14>&#39;x&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sqy&#39;</span>, Square, (<span style=color:#d14>&#39;y&#39;</span>,))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;ax&#39;</span>, Multiply, (<span style=color:#d14>&#39;a&#39;</span>, <span style=color:#d14>&#39;x&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;by&#39;</span>, Multiply, (<span style=color:#d14>&#39;b&#39;</span>, <span style=color:#d14>&#39;y&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;sqxpsqy&#39;</span>, Add, (<span style=color:#d14>&#39;sqx&#39;</span>, <span style=color:#d14>&#39;sqy&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;axpby&#39;</span>, Add, (<span style=color:#d14>&#39;ax&#39;</span>, <span style=color:#d14>&#39;by&#39;</span>))
</span></span><span style=display:flex><span>cg<span style=color:#000;font-weight:700>.</span>add_computation_node(<span style=color:#d14>&#39;f&#39;</span>, Add, (<span style=color:#d14>&#39;sqxpsqy&#39;</span>, <span style=color:#d14>&#39;axpby&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>0</span>, <span style=color:#099>50</span>):
</span></span><span style=display:flex><span>    cg<span style=color:#000;font-weight:700>.</span>forward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>    cg<span style=color:#000;font-weight:700>.</span>backward(<span style=color:#d14>&#39;f&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dx <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>    dy <span style=color:#000;font-weight:700>=</span> cg<span style=color:#000;font-weight:700>.</span>get_gradient(<span style=color:#d14>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>    cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;x&#39;</span>]<span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>-=</span> dx <span style=color:#000;font-weight:700>*</span> <span style=color:#099>0.1</span>
</span></span><span style=display:flex><span>    cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;y&#39;</span>]<span style=color:#000;font-weight:700>.</span>output <span style=color:#000;font-weight:700>-=</span> dy <span style=color:#000;font-weight:700>*</span> <span style=color:#099>0.1</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#39;Minimum:&#39;</span>, cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;x&#39;</span>]<span style=color:#000;font-weight:700>.</span>output, cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;y&#39;</span>]<span style=color:#000;font-weight:700>.</span>output)
</span></span></code></pre></div><pre><code>Minimum: -199.9971455046146 -3.9999429100922916
</code></pre><p>All done! Minimum was correctly found, it is time to visual assess it.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>matplotlib.pylab</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>plt</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>%</span>matplotlib inline
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>%</span>config InlineBackend<span style=color:#000;font-weight:700>.</span>figure_format<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;retina&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x, y <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>meshgrid(np<span style=color:#000;font-weight:700>.</span>arange(<span style=color:#000;font-weight:700>-</span><span style=color:#099>800</span>, <span style=color:#099>800</span>), np<span style=color:#000;font-weight:700>.</span>arange(<span style=color:#000;font-weight:700>-</span><span style=color:#099>400</span>, <span style=color:#099>400</span>))
</span></span><span style=display:flex><span>z <span style=color:#000;font-weight:700>=</span> x<span style=color:#000;font-weight:700>**</span><span style=color:#099>2</span> <span style=color:#000;font-weight:700>+</span> y<span style=color:#000;font-weight:700>**</span><span style=color:#099>2</span> <span style=color:#000;font-weight:700>+</span> <span style=color:#099>400</span><span style=color:#000;font-weight:700>*</span>x <span style=color:#000;font-weight:700>+</span> <span style=color:#099>8</span><span style=color:#000;font-weight:700>*</span>y 
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>3</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>imshow(z, extent<span style=color:#000;font-weight:700>=</span>[x<span style=color:#000;font-weight:700>.</span>min(), x<span style=color:#000;font-weight:700>.</span>max(), y<span style=color:#000;font-weight:700>.</span>min(), y<span style=color:#000;font-weight:700>.</span>max()], origin<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#34;lower&#34;</span>, cmap<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;cool&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter([cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;x&#39;</span>]<span style=color:#000;font-weight:700>.</span>output], [cg<span style=color:#000;font-weight:700>.</span>inputs[<span style=color:#d14>&#39;y&#39;</span>]<span style=color:#000;font-weight:700>.</span>output], c <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;red&#39;</span>, label<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;Min&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;$x$&#39;</span>, ylabel<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span></code></pre></div></div></div><p><img src=output_23_1.png#center alt=png></p><p>The code is available here:
<a href=https://github.com/mikoff/blog_projects/tree/master/auto_differentation class=external-link target=_blank rel=noopener>https://github.com/mikoff/blog_projects/tree/master/auto_differentation</a></p><h2 id=references>References
<a class=heading-link href=#references><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.deeplearningbook.org/ class=external-link target=_blank rel=noopener>https://www.deeplearningbook.org/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://cs231n.github.io/optimization-2/ class=external-link target=_blank rel=noopener>https://cs231n.github.io/optimization-2/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>¬©
2024
Aleksandr Mikoff
¬∑
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>