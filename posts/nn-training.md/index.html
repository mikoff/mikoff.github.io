<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Aleksandr Mikoff"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Recently, I have been reading a new book by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD."><meta property="og:title" content><meta property="og:description" content="Recently, I have been reading a new book by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/nn-training.md/"><meta property="article:section" content="posts"><base href=https://mikoff.github.io/posts/nn-training.md/><title>Â· Aleksandr Mikoff's blog</title><link rel=canonical href=https://mikoff.github.io/posts/nn-training.md/><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.11.2/css/all.css integrity=sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin=anonymous><link rel=stylesheet href=https://mikoff.github.io/fontdata/css/academicons.min.css><link rel=stylesheet href=https://mikoff.github.io/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/coder-dark.min.83a2010dac9f59f943b3004cd6c4f230507ad036da635d3621401d42ec4e2835.css integrity="sha256-g6IBDayfWflDswBM1sTyMFB60DbaY102IUAdQuxOKDU=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/image.css><link rel=stylesheet href=https://mikoff.github.io/css/spoiler.css><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-16x16.png sizes=16x16><meta name=generator content="Hugo 0.119.0"></head><body class=colorscheme-auto><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fas fa-bars"></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fas fa-calendar"></i>
<time datetime=0001-01-01T00:00:00Z>January 1, 0001</time></span>
<span class=reading-time><i class="fas fa-clock"></i>
8-minute read</span></div></div></header><div><p>Recently, I have been reading a new <a href=https://udlbook.github.io/udlbook/>book</a> by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.</p><p>This post is:</p><ol><li>a collection of keynotes from the first seven chapters of the book, that I have found useful for myself;</li><li>the <code>numpy-only</code> implementation of deep neural network with variable layers size and training using SGD.</li></ol><h2 id=general-terms>General terms</h2><p>The <strong>Universal Approximation Theorem</strong> proves that for any continuous function, there exists a network that can approximate this function to any specified precision.</p><p><strong>Shallow Neural Networks</strong> (SNN) approximate the required function using piecewise linear functions of the input.</p><p>The <strong>Dimensionality Curse</strong> is a well-known problem for Particle Filters. In neural networks, this problem is also known: as the input dimensions grow, the number of linear regions increases rapidly, leading to the need for a larger number of parameters in the neural network to obtain a good approximation of the underlying function.</p><h2 id=layers>Layers</h2><p>Neural networks are often described in terms of <em>layers</em>:</p><ol><li>input layer;</li><li>hidden layer;</li><li>output layer.</li></ol><p>When data is passed through the neural network, the values that are fed to the <em>hidden layer</em> are termed <em>pre-activations</em> (e.g., before ReLU is applied). The values after the <em>hidden layer</em>, or after ReLU is applied, are called <em>activations.</em></p><p>Here is what the <a href=https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu>ReLU</a> layer does for 1D-function using SNN:</p><ol><li>Generates $D$ linear functions, that are <em>pre-activations</em>.</li><li>For each pre-activation, ReLU clamps the input, so its range becomes $[0, +\inf)$; these are the activations.</li><li>Then each <em>activation</em> is multiplied by a number, and all of them are summed up with the bias, which controls the height.</li></ol><h2 id=deep-neural-networks>Deep Neural networks</h2><p>Since the approximation power of SNN depends on the number of hidden units, for some high-dimensional or complex functions, the required number of hidden units is impractically large. Deep networks produce many more linear regions for the same number of parameters and, as a result, have better descriptive power. For example, the number of linear regions for an SNN with 6 hidden units is equal to 7, but a Deep Neural Network (DNN) consisting of two layers with 3 hidden units each creates 9 linear regions.</p><p><em>Note:</em> Deep networks can create an extremely large number of linear regions, but these contain complex dependencies and symmetries.</p><h3 id=hyperparameters>Hyperparameters</h3><ul><li><strong>width</strong> - the number of hidden units in each layer ($D$);</li><li><strong>depth</strong> - the number of hidden layers ($K$);</li><li><strong>capacity</strong> - the total number of hidden units.</li></ul><h3 id=matrix-notation>Matrix notation</h3><p>$$
\begin{split}
\mathbf{h}_1 &= \mathbf{a}[\boldsymbol{\beta}_0 + \boldsymbol{\Omega}_0 \mathbf{x}] \\
\mathbf{h}_2 &= \mathbf{a}[\boldsymbol{\beta}_1 + \boldsymbol{\Omega}_1 \mathbf{h}_1] \\
&~~\vdots \\
\mathbf{h}_{K} &= \mathbf{a}[\boldsymbol{\beta}_{K-1} + \boldsymbol{\Omega}_{K-1} \mathbf{h}_{K-1}] \\
\mathbf{y} &= \boldsymbol{\beta}_K + \boldsymbol{\Omega}_K \mathbf{h}_{K}
\end{split}
$$
where $\mathbf{x}$ is the input vector, $\boldsymbol{\beta}$ - bias vector, which size is equal to $D_k$, $\boldsymbol{\Omega}$ - weight matrix.</p><p>One general equation for the whole neural network is:
$$
\mathbf{y} = f[\mathbf{x}, \boldsymbol{\phi}] ,
$$
where $\boldsymbol{\phi}$ are the parameters of the model, comprising all the weight matrices and bias vectors $\boldsymbol{\phi} = {\boldsymbol{\beta}_k, \boldsymbol{\Omega}_k}_{k=0}^K$.</p><h3 id=fitting>Fitting</h3><p>There are two most popular options for NN training:</p><ul><li><p><strong>Stochastic Gradient Descent (SGD)</strong> adds randomness at any given iteration. The mechanism is simple: the algorithm chooses a random subset of data, known as a minibatch, and computes the gradient from this training subset. The batches are usually drawn without replacement. An alternative interpretation of SGD is that it computes the gradient of a different loss function at each iteration; the loss function depends on both the model and the training data and hence will differ for each randomly selected batch. In this view, SGD performs deterministic gradient descent on a constantly changing loss function.</p></li><li><p><strong>ADAM</strong>, adaptive moment estimation, applies moments, or smoothing, both to the estimated gradient and normalizing term (that is the squared gradient).</p></li></ul><h3 id=initialization>Initialization</h3><p>Usually the biases are initialized to zero, and the weights as normally distributed with zero mean and variance $\sigma_\Omega^2$.
<em>He initialization</em> allows to have the variance of the subsequent pre-activations to be the same as variance of the original pre-activations. To achieve this the variance should be defined as:
$$
\sigma_\Omega^2 = \frac{2}{D_h},
$$
$D_h$ is the dimension of the original layer. The details can be found <a href=https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4>here</a>.</p><h2 id=code-example>Code example</h2><p>Let&rsquo;s consider the following neural network $\mathbf{f}[\mathbf{x}, \boldsymbol{\phi}]$ with input $\mathbf{x}$, parameters $\boldsymbol{\phi}$ and two hidden layers $\mathbf{h}_1, \mathbf{h}_2$:</p><p><img src=nn-training-1.drawio.svg#center alt=nn-1></p><p>The model parameters $\boldsymbol{\phi} = {\boldsymbol{\beta}_0, \boldsymbol{\Omega}_0, \boldsymbol{\beta}_1, \boldsymbol{\Omega}_1 , \boldsymbol{\beta}_2, \boldsymbol{\Omega}_2 }$.</p><h3 id=computing-derivatives>Computing derivatives</h3><p>The derivative tells us how the loss changes if we make a small change to the parameters. <em>Optimization</em> algorithms use this information to update the parameters so that the loss becomes smaller.</p><p>Each layer involves the multiplication of the weights and activations from the previous layer. So, any change to the weight affects the activation at the hidden unit. During the <em>forward pass</em>, the activations are stored to compute the gradients later.</p><p>Once all the data have moved forward, we can calculate the loss and roll this loss back to each and every parameter in our network. This step is known as the <em>backward pass</em>.</p><p>In the following chart, you can see the forward and backward passes for our example neural network. First, we sequentially calculate the actual values of $\mathbf{f}_k$ and $\mathbf{h}_k$, obtaining the loss value at the end.</p><p><img src=nn-training-2.drawio.svg#center alt=nn-2></p><p>In our example let&rsquo;s use the least squares loss: $\mathbf{L}(\mathbf{f}_o, \mathbf{y}_i) = (\mathbf{f}_o - \mathbf{y}_i)^2$, whose derivative is well known.
Having the numerical value of the derivative of the loss, we can roll it back to all the parameters using the chain rule. For example:</p><p>$$
\frac{\partial \mathcal{L}}{\partial\mathbf{h}_1} = \frac{\partial \mathbf{f}_1}{\partial\mathbf{h}_1} \left( \frac{\partial \mathbf{h}_2}{\partial\mathbf{f}_1} \frac{\partial \mathbf{f}_2}{\partial\mathbf{h}_2} \frac{\partial \mathcal{L}}{\partial\mathbf{f}_2} \right).
$$
Please note that since matrix multiplication is not commutative, this is the only correct order. The term in brackets is computed in the previous step and can be directly reused.</p><h3 id=actual-code>Actual code</h3><p>Now, let&rsquo;s define our neural network layers and the general neural network class:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>import</span> <span style=color:#0cf;font-weight:700>numpy</span> <span style=color:#069;font-weight:700>as</span> <span style=color:#0cf;font-weight:700>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>HiddenLayer</span>:
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self, n_units, input_size, learning_rate):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>omega <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>normal(<span style=color:#f60>0</span>, np<span style=color:#555>.</span>sqrt(<span style=color:#f60>2</span><span style=color:#555>/</span>(n_units)), size<span style=color:#555>=</span>(n_units, input_size))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>beta <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros((n_units, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>alpha <span style=color:#555>=</span> learning_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward</span>(self, <span style=color:#366>input</span>):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>h <span style=color:#555>=</span> np<span style=color:#555>.</span>clip(<span style=color:#366>input</span>, <span style=color:#f60>0</span>, <span style=color:#069;font-weight:700>None</span>)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>f <span style=color:#555>=</span> self<span style=color:#555>.</span>beta <span style=color:#555>+</span> self<span style=color:#555>.</span>omega <span style=color:#555>@</span> self<span style=color:#555>.</span>h
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dloss):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>df_dbeta <span style=color:#555>=</span> np<span style=color:#555>.</span>eye(<span style=color:#366>len</span>(self<span style=color:#555>.</span>beta))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>df_domega <span style=color:#555>=</span> np<span style=color:#555>.</span>copy(self<span style=color:#555>.</span>h<span style=color:#555>.</span>T)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>df_dh <span style=color:#555>=</span> np<span style=color:#555>.</span>copy(self<span style=color:#555>.</span>omega<span style=color:#555>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>dloss_dbeta <span style=color:#555>=</span> self<span style=color:#555>.</span>df_dbeta <span style=color:#555>@</span> dloss
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>dloss_domega <span style=color:#555>=</span> dloss <span style=color:#555>@</span> self<span style=color:#555>.</span>df_domega
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>dloss_dh <span style=color:#555>=</span> self<span style=color:#555>.</span>df_dh <span style=color:#555>@</span> dloss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>omega <span style=color:#555>-=</span> self<span style=color:#555>.</span>alpha <span style=color:#555>*</span> self<span style=color:#555>.</span>dloss_domega
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>beta <span style=color:#555>-=</span> self<span style=color:#555>.</span>alpha <span style=color:#555>*</span> self<span style=color:#555>.</span>dloss_dbeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>InputLayer</span>:
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self, n_units, input_size, learning_rate):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>omega <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>normal(<span style=color:#f60>0</span>, np<span style=color:#555>.</span>sqrt(<span style=color:#f60>2</span><span style=color:#555>/</span>(n_units)), size<span style=color:#555>=</span>(n_units, input_size))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>beta <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros((n_units, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>alpha <span style=color:#555>=</span> learning_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward</span>(self, <span style=color:#366>input</span>):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>input <span style=color:#555>=</span> np<span style=color:#555>.</span>asarray(<span style=color:#366>input</span>)<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>f <span style=color:#555>=</span> self<span style=color:#555>.</span>beta <span style=color:#555>+</span> self<span style=color:#555>.</span>omega <span style=color:#555>@</span> self<span style=color:#555>.</span>input
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> self<span style=color:#555>.</span>f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, dloss):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>df_dbeta <span style=color:#555>=</span> np<span style=color:#555>.</span>eye(<span style=color:#366>len</span>(self<span style=color:#555>.</span>beta))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>df_domega <span style=color:#555>=</span> np<span style=color:#555>.</span>copy(self<span style=color:#555>.</span>input<span style=color:#555>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>dloss_dbeta <span style=color:#555>=</span> self<span style=color:#555>.</span>df_dbeta <span style=color:#555>@</span> dloss
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>dloss_domega <span style=color:#555>=</span> dloss <span style=color:#555>@</span> self<span style=color:#555>.</span>df_domega
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>omega <span style=color:#555>-=</span> self<span style=color:#555>.</span>alpha <span style=color:#555>*</span> self<span style=color:#555>.</span>dloss_domega
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>beta <span style=color:#555>-=</span> self<span style=color:#555>.</span>alpha <span style=color:#555>*</span> self<span style=color:#555>.</span>dloss_dbeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>class</span> <span style=color:#0a8;font-weight:700>NN</span>:
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> __init__(self, input_size, layers_sizes, output_size, learning_rate):
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>input_layer <span style=color:#555>=</span> InputLayer(layers_sizes[<span style=color:#f60>0</span>], input_size, learning_rate)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>hidden_layers <span style=color:#555>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> l_input, l_output <span style=color:#000;font-weight:700>in</span> <span style=color:#366>zip</span>(layers_sizes[<span style=color:#f60>0</span>:], layers_sizes[<span style=color:#f60>1</span>:]):
</span></span><span style=display:flex><span>            self<span style=color:#555>.</span>hidden_layers<span style=color:#555>.</span>append(HiddenLayer(l_output, l_input, learning_rate))
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output_layer <span style=color:#555>=</span> HiddenLayer(output_size, layers_sizes[<span style=color:#555>-</span><span style=color:#f60>1</span>], learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>forward</span>(self, <span style=color:#366>input</span>):
</span></span><span style=display:flex><span>        o <span style=color:#555>=</span> self<span style=color:#555>.</span>input_layer<span style=color:#555>.</span>forward(<span style=color:#366>input</span>)
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> l <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>hidden_layers:
</span></span><span style=display:flex><span>            o <span style=color:#555>=</span> l<span style=color:#555>.</span>forward(o)
</span></span><span style=display:flex><span>        o <span style=color:#555>=</span> self<span style=color:#555>.</span>output_layer<span style=color:#555>.</span>forward(o)
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>return</span> o
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>backward</span>(self, predicted_value, true_value):
</span></span><span style=display:flex><span>        dloss <span style=color:#555>=</span> <span style=color:#f60>2</span> <span style=color:#555>*</span> (predicted_value <span style=color:#555>-</span> true_value)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>output_layer<span style=color:#555>.</span>backward(dloss)
</span></span><span style=display:flex><span>        dfprev_dhprev <span style=color:#555>=</span> self<span style=color:#555>.</span>output_layer<span style=color:#555>.</span>df_dh
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#069;font-weight:700>for</span> l <span style=color:#000;font-weight:700>in</span> self<span style=color:#555>.</span>hidden_layers[::<span style=color:#555>-</span><span style=color:#f60>1</span>]:
</span></span><span style=display:flex><span>            dhprev_df <span style=color:#555>=</span> np<span style=color:#555>.</span>diag((l<span style=color:#555>.</span>f <span style=color:#555>&gt;</span> <span style=color:#f60>0</span>)<span style=color:#555>.</span>flatten())<span style=color:#555>.</span>astype(<span style=color:#366>int</span>)
</span></span><span style=display:flex><span>            dloss <span style=color:#555>=</span> dhprev_df <span style=color:#555>@</span> dfprev_dhprev <span style=color:#555>@</span> dloss
</span></span><span style=display:flex><span>            l<span style=color:#555>.</span>backward(dloss)
</span></span><span style=display:flex><span>            dfprev_dhprev <span style=color:#555>=</span> l<span style=color:#555>.</span>df_dh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        dhprev_df <span style=color:#555>=</span> np<span style=color:#555>.</span>diag((self<span style=color:#555>.</span>input_layer<span style=color:#555>.</span>f <span style=color:#555>&gt;</span> <span style=color:#f60>0</span>)<span style=color:#555>.</span>flatten())<span style=color:#555>.</span>astype(<span style=color:#366>int</span>)
</span></span><span style=display:flex><span>        dloss <span style=color:#555>=</span> dhprev_df <span style=color:#555>@</span> dfprev_dhprev <span style=color:#555>@</span> dloss
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>input_layer<span style=color:#555>.</span>backward(dloss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>train</span>(self, <span style=color:#366>input</span>, true_value):
</span></span><span style=display:flex><span>        p <span style=color:#555>=</span> self<span style=color:#555>.</span>forward(<span style=color:#366>input</span>)
</span></span><span style=display:flex><span>        self<span style=color:#555>.</span>backward(p, true_value)
</span></span></code></pre></div></div></div><p>It&rsquo;s the time to define our first function $f(x) = \sin(10 x) + 0.75$ and train our NN to approximate it:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>import</span> <span style=color:#0cf;font-weight:700>numpy</span> <span style=color:#069;font-weight:700>as</span> <span style=color:#0cf;font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>import</span> <span style=color:#0cf;font-weight:700>matplotlib.pylab</span> <span style=color:#069;font-weight:700>as</span> <span style=color:#0cf;font-weight:700>plt</span>
</span></span><span style=display:flex><span><span style=color:#555>%</span>matplotlib inline
</span></span><span style=display:flex><span><span style=color:#555>%</span>config InlineBackend<span style=color:#555>.</span>figure_format<span style=color:#555>=</span><span style=color:#c30>&#39;retina&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># our function that will be approximated by our NN</span>
</span></span><span style=display:flex><span>f <span style=color:#555>=</span> <span style=color:#069;font-weight:700>lambda</span> x: np<span style=color:#555>.</span>sin(x <span style=color:#555>*</span> <span style=color:#f60>10</span>) <span style=color:#555>+</span> <span style=color:#f60>0.75</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># NN structure - 3 layers, each having 12 units, input and output are 1d</span>
</span></span><span style=display:flex><span>learning_rate <span style=color:#555>=</span> <span style=color:#f60>0.01</span>
</span></span><span style=display:flex><span>nn <span style=color:#555>=</span> NN(<span style=color:#f60>1</span>, (<span style=color:#f60>24</span>, <span style=color:#f60>24</span>, <span style=color:#f60>24</span>), <span style=color:#f60>1</span>, learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># generate 10000 samples and train NN</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>for</span> b <span style=color:#000;font-weight:700>in</span> <span style=color:#366>range</span>(<span style=color:#f60>0</span>, <span style=color:#f60>10000</span>):
</span></span><span style=display:flex><span>    x <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>uniform(<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>, size <span style=color:#555>=</span> (<span style=color:#f60>1</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>    y <span style=color:#555>=</span> f(<span style=color:#555>*</span>x)
</span></span><span style=display:flex><span>    nn<span style=color:#555>.</span>train(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># now let&#39;s get true values and predictions for some range</span>
</span></span><span style=display:flex><span>x <span style=color:#555>=</span> np<span style=color:#555>.</span>linspace(<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>100</span>)
</span></span><span style=display:flex><span>y_predict <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros_like(x)
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>for</span> i, xx <span style=color:#000;font-weight:700>in</span> <span style=color:#366>enumerate</span>(x):
</span></span><span style=display:flex><span>    y_predict[i] <span style=color:#555>=</span> nn<span style=color:#555>.</span>forward(xx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># visualize </span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>3</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(x, f(x), label <span style=color:#555>=</span> <span style=color:#c30>&#34;True function&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(x, y_predict, label <span style=color:#555>=</span> <span style=color:#c30>&#34;NN approximation&#34;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel<span style=color:#555>=</span><span style=color:#c30>&#39;$x$&#39;</span>, ylabel<span style=color:#555>=</span><span style=color:#c30>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend(loc<span style=color:#555>=</span><span style=color:#c30>&#39;upper left&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_13_1.png#center alt=png></p><p>Let&rsquo;s do the same for the second function, which is 2d:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># our function that will be approximated by our NN</span>
</span></span><span style=display:flex><span>f <span style=color:#555>=</span> <span style=color:#069;font-weight:700>lambda</span> x, y: <span style=color:#f60>25</span> <span style=color:#555>*</span> x <span style=color:#555>+</span> <span style=color:#f60>17.8</span> <span style=color:#555>*</span> y <span style=color:#555>+</span> <span style=color:#f60>0.25</span> <span style=color:#555>*</span> x <span style=color:#555>*</span> y <span style=color:#555>+</span> x <span style=color:#555>*</span> x <span style=color:#555>*</span> <span style=color:#f60>15.0</span> <span style=color:#555>-</span> y <span style=color:#555>*</span> y <span style=color:#555>*</span> <span style=color:#f60>5.0</span> <span style=color:#555>-</span> <span style=color:#f60>25</span> <span style=color:#555>*</span> (x <span style=color:#555>&lt;</span> <span style=color:#f60>0.0</span> <span style=color:#000;font-weight:700>and</span> y <span style=color:#555>&lt;</span> <span style=color:#f60>0.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># NN structure - 2 layers, each having 12 units, input is 2d, output is 1d</span>
</span></span><span style=display:flex><span>learning_rate <span style=color:#555>=</span> <span style=color:#f60>0.001</span>
</span></span><span style=display:flex><span>nn <span style=color:#555>=</span> NN(<span style=color:#f60>2</span>, (<span style=color:#f60>12</span>, <span style=color:#f60>24</span>), <span style=color:#f60>1</span>, learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># generate 10000 samples and train NN</span>
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>for</span> b <span style=color:#000;font-weight:700>in</span> <span style=color:#366>range</span>(<span style=color:#f60>0</span>, <span style=color:#f60>10000</span>):
</span></span><span style=display:flex><span>    x <span style=color:#555>=</span> np<span style=color:#555>.</span>random<span style=color:#555>.</span>uniform(<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>, size <span style=color:#555>=</span> (<span style=color:#f60>2</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>    y <span style=color:#555>=</span> f(<span style=color:#555>*</span>x)
</span></span><span style=display:flex><span>    nn<span style=color:#555>.</span>train(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># now let&#39;s get true values and predictions for some range</span>
</span></span><span style=display:flex><span>x, y <span style=color:#555>=</span> np<span style=color:#555>.</span>arange(<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>0.05</span>), np<span style=color:#555>.</span>arange(<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>0.05</span>)
</span></span><span style=display:flex><span>y_predict <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros((x<span style=color:#555>.</span>shape[<span style=color:#f60>0</span>], y<span style=color:#555>.</span>shape[<span style=color:#f60>0</span>]))
</span></span><span style=display:flex><span>y_true <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros((x<span style=color:#555>.</span>shape[<span style=color:#f60>0</span>], y<span style=color:#555>.</span>shape[<span style=color:#f60>0</span>]))
</span></span><span style=display:flex><span><span style=color:#069;font-weight:700>for</span> i, xx <span style=color:#000;font-weight:700>in</span> <span style=color:#366>enumerate</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>for</span> j, yy <span style=color:#000;font-weight:700>in</span> <span style=color:#366>enumerate</span>(y):
</span></span><span style=display:flex><span>        y_predict[i, j] <span style=color:#555>=</span> nn<span style=color:#555>.</span>forward([xx, yy])
</span></span><span style=display:flex><span>        y_true[i, j] <span style=color:#555>=</span> f(xx, yy)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#09f;font-style:italic># visualize </span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>2</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>3</span>))
</span></span><span style=display:flex><span>ax[<span style=color:#f60>0</span>]<span style=color:#555>.</span>imshow(y_true, extent<span style=color:#555>=</span>[x<span style=color:#555>.</span>min(), x<span style=color:#555>.</span>max(), y<span style=color:#555>.</span>min(), y<span style=color:#555>.</span>max()], origin<span style=color:#555>=</span><span style=color:#c30>&#34;lower&#34;</span>, cmap<span style=color:#555>=</span><span style=color:#c30>&#39;cool&#39;</span>, vmin<span style=color:#555>=</span>np<span style=color:#555>.</span>min(y_true), vmax<span style=color:#555>=</span>np<span style=color:#555>.</span>max(y_true))
</span></span><span style=display:flex><span>ax[<span style=color:#f60>0</span>]<span style=color:#555>.</span>set(xlabel<span style=color:#555>=</span><span style=color:#c30>&#39;$x$&#39;</span>, ylabel<span style=color:#555>=</span><span style=color:#c30>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#f60>0</span>]<span style=color:#555>.</span>set_title(<span style=color:#c30>&#34;True function&#34;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#f60>1</span>]<span style=color:#555>.</span>imshow(y_predict, extent<span style=color:#555>=</span>[x<span style=color:#555>.</span>min(), x<span style=color:#555>.</span>max(), y<span style=color:#555>.</span>min(), y<span style=color:#555>.</span>max()], origin<span style=color:#555>=</span><span style=color:#c30>&#34;lower&#34;</span>, cmap<span style=color:#555>=</span><span style=color:#c30>&#39;cool&#39;</span>, vmin<span style=color:#555>=</span>np<span style=color:#555>.</span>min(y_true), vmax<span style=color:#555>=</span>np<span style=color:#555>.</span>max(y_true))
</span></span><span style=display:flex><span>ax[<span style=color:#f60>1</span>]<span style=color:#555>.</span>set(xlabel<span style=color:#555>=</span><span style=color:#c30>&#39;$x$&#39;</span>, ylabel<span style=color:#555>=</span><span style=color:#c30>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#f60>1</span>]<span style=color:#555>.</span>set_title(<span style=color:#c30>&#34;NN approximation&#34;</span>)
</span></span><span style=display:flex><span>fig<span style=color:#555>.</span>tight_layout()
</span></span></code></pre></div></div></div><p><img src=output_15_0.png#center alt=png></p><p>Bingo!</p></div><footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>Â© 2023
Aleksandr Mikoff
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.slim.min.js></script></main></body></html>