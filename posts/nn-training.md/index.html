<!doctype html><html lang=en><head><title>Understanding deep learning: training, SGD, code samples · Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Recently, I have been reading a new [book]1 by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding deep learning: training, SGD, code samples"><meta name=twitter:description content="Recently, I have been reading a new [book]1 by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD."><meta property="og:title" content="Understanding deep learning: training, SGD, code samples"><meta property="og:description" content="Recently, I have been reading a new [book]1 by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/nn-training.md/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-15T12:00:00+03:00"><meta property="article:modified_time" content="2023-10-15T12:00:00+03:00"><link rel=canonical href=https://mikoff.github.io/posts/nn-training.md/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://mikoff.github.io/posts/nn-training.md/>Understanding deep learning: training, SGD, code samples</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2023-10-15T12:00:00+03:00>October 15, 2023
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
8-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/neural-networks/>Neural Networks</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/reading/>Reading</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/deep-learning/>Deep Learning</a></span></div></div></header><div class=post-content><p>Recently, I have been reading a new [book]<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> by S. Prince titled &ldquo;Understanding Deep Learning.&rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.</p><p>This post is:</p><ol><li>a collection of keynotes from the first seven chapters of the book, that I have found useful for myself;</li><li>the <code>numpy-only</code> implementation of deep neural network with variable layers size and training using SGD.</li></ol><h2 id=general-terms>General terms
<a class=heading-link href=#general-terms><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The <strong>Universal Approximation Theorem</strong> proves that for any continuous function, there exists a network that can approximate this function to any specified precision.</p><p><strong>Shallow Neural Networks</strong> (SNN) approximate the required function using piecewise linear functions of the input.</p><p>The <strong>Dimensionality Curse</strong> is a well-known problem for Particle Filters. In neural networks, this problem is also known: as the input dimensions grow, the number of linear regions increases rapidly, leading to the need for a larger number of parameters in the neural network to obtain a good approximation of the underlying function.</p><h2 id=layers>Layers
<a class=heading-link href=#layers><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Neural networks are often described in terms of <em>layers</em>:</p><ol><li>input layer;</li><li>hidden layer;</li><li>output layer.</li></ol><p>When data is passed through the neural network, the values that are fed to the <em>hidden layer</em> are termed <em>pre-activations</em> (e.g., before ReLU is applied). The values after the <em>hidden layer</em>, or after ReLU is applied, are called <em>activations.</em></p><p>Here is what the [ReLU]<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> layer does for 1D-function using SNN:</p><ol><li>Generates $D$ linear functions, that are <em>pre-activations</em>.</li><li>For each pre-activation, ReLU clamps the input, so its range becomes $[0, +\inf)$; these are the activations.</li><li>Then each <em>activation</em> is multiplied by a number, and all of them are summed up with the bias, which controls the height.</li></ol><h2 id=deep-neural-networks>Deep Neural networks
<a class=heading-link href=#deep-neural-networks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Since the approximation power of SNN depends on the number of hidden units, for some high-dimensional or complex functions, the required number of hidden units is impractically large. Deep networks produce many more linear regions for the same number of parameters and, as a result, have better descriptive power. For example, the number of linear regions for an SNN with 6 hidden units is equal to 7, but a Deep Neural Network (DNN) consisting of two layers with 3 hidden units each creates 9 linear regions.</p><p><em>Note:</em> Deep networks can create an extremely large number of linear regions, but these contain complex dependencies and symmetries.</p><h3 id=hyperparameters>Hyperparameters
<a class=heading-link href=#hyperparameters><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><strong>width</strong> - the number of hidden units in each layer ($D$);</li><li><strong>depth</strong> - the number of hidden layers ($K$);</li><li><strong>capacity</strong> - the total number of hidden units.</li></ul><h3 id=matrix-notation>Matrix notation
<a class=heading-link href=#matrix-notation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3>$$
\begin{split}
\mathbf{h}_1 &= \mathbf{a}[\boldsymbol{\beta}_0 + \boldsymbol{\Omega}_0 \mathbf{x}] \\\\
\mathbf{h}_2 &= \mathbf{a}[\boldsymbol{\beta}_1 + \boldsymbol{\Omega}_1 \mathbf{h}_1] \\\\
&~~\vdots \\\\
\mathbf{h}_{K} &= \mathbf{a}[\boldsymbol{\beta}_{K-1} + \boldsymbol{\Omega}_{K-1} \mathbf{h}_{K-1}] \\\\
\mathbf{y} &= \boldsymbol{\beta}_K + \boldsymbol{\Omega}_K \mathbf{h}_{K}
\end{split}
$$<p>where $\mathbf{x}$ is the input vector, $\boldsymbol{\beta}$ - bias vector, which size is equal to $D_k$, $\boldsymbol{\Omega}$ - weight matrix.</p><p>One general equation for the whole neural network is:</p>$$
\mathbf{y} = f[\mathbf{x}, \boldsymbol{\phi}] ,
$$<p>where $\boldsymbol{\phi}$ are the parameters of the model, comprising all the weight matrices and bias vectors $\boldsymbol{\phi} = \{\boldsymbol{\beta}_k, \boldsymbol{\Omega}_k\}_{k=0}^K$.</p><h3 id=fitting>Fitting
<a class=heading-link href=#fitting><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>There are two most popular options for NN training:</p><ul><li><p><strong>Stochastic Gradient Descent (SGD)</strong> adds randomness at any given iteration. The mechanism is simple: the algorithm chooses a random subset of data, known as a minibatch, and computes the gradient from this training subset. The batches are usually drawn without replacement. An alternative interpretation of SGD is that it computes the gradient of a different loss function at each iteration; the loss function depends on both the model and the training data and hence will differ for each randomly selected batch. In this view, SGD performs deterministic gradient descent on a constantly changing loss function.</p></li><li><p><strong>ADAM</strong>, adaptive moment estimation, applies moments, or smoothing, both to the estimated gradient and normalizing term (that is the squared gradient).</p></li></ul><h3 id=initialization>Initialization
<a class=heading-link href=#initialization><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Usually the biases are initialized to zero, and the weights as normally distributed with zero mean and variance $\sigma_\Omega^2$.
<em>He initialization</em> allows to have the variance of the subsequent pre-activations to be the same as variance of the original pre-activations. To achieve this the variance should be defined as:</p>$$
\sigma_\Omega^2 = \frac{2}{D_h},
$$<p>$D_h$ is the dimension of the original layer. The details can be found [there]<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><h2 id=code-example>Code example
<a class=heading-link href=#code-example><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Let&rsquo;s consider the following neural network $\mathbf{f}[\mathbf{x}, \boldsymbol{\phi}]$ with input $\mathbf{x}$, parameters $\boldsymbol{\phi}$ and two hidden layers $\mathbf{h}_1, \mathbf{h}_2$:</p><p><img src=nn-training-1.drawio.svg#center alt=nn-1></p><p>The model parameters $\boldsymbol{\phi} = \{\boldsymbol{\beta}_0, \boldsymbol{\Omega}_0, \boldsymbol{\beta}_1, \boldsymbol{\Omega}_1 , \boldsymbol{\beta}_2, \boldsymbol{\Omega}_2 \}$.</p><h3 id=computing-derivatives>Computing derivatives
<a class=heading-link href=#computing-derivatives><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The derivative tells us how the loss changes if we make a small change to the parameters. <em>Optimization</em> algorithms use this information to update the parameters so that the loss becomes smaller.</p><p>Each layer involves the multiplication of the weights and activations from the previous layer. So, any change to the weight affects the activation at the hidden unit. During the <em>forward pass</em>, the activations are stored to compute the gradients later.</p><p>Once all the data have moved forward, we can calculate the loss and roll this loss back to each and every parameter in our network. This step is known as the <em>backward pass</em>.</p><p>In the following chart, you can see the forward and backward passes for our example neural network. First, we sequentially calculate the actual values of $\mathbf{f}_k$ and $\mathbf{h}_k$, obtaining the loss value at the end.</p><p><img src=nn-training-2.drawio.svg#center alt=nn-2></p><p>In our example let&rsquo;s use the least squares loss: $\mathbf{L}(\mathbf{f}_o, \mathbf{y}_i) = (\mathbf{f}_o - \mathbf{y}_i)^2$, whose derivative is well known.
Having the numerical value of the derivative of the loss, we can roll it back to all the parameters using the chain rule. For example:</p>$$
\frac{\partial \mathcal{L}}{\partial\mathbf{h}_1} = \frac{\partial \mathbf{f}_1}{\partial\mathbf{h}_1} \left( \frac{\partial \mathbf{h}_2}{\partial\mathbf{f}_1} \frac{\partial \mathbf{f}_2}{\partial\mathbf{h}_2} \frac{\partial \mathcal{L}}{\partial\mathbf{f}_2} \right).
$$<p>Please note that since matrix multiplication is not commutative, this is the only correct order. The term in brackets is computed in the previous step and can be directly reused.</p><h3 id=actual-code>Actual code
<a class=heading-link href=#actual-code><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Now, let&rsquo;s define our neural network layers and the general neural network class:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fff;font-weight:700>import</span> numpy <span style=color:#fff;font-weight:700>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>class</span> HiddenLayer:
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> __init__(self, n_units, input_size, learning_rate):
</span></span><span style=display:flex><span>        self.omega = np.random.normal(<span style=color:#ff0;font-weight:700>0</span>, np.sqrt(<span style=color:#ff0;font-weight:700>2</span>/(n_units)), size=(n_units, input_size))
</span></span><span style=display:flex><span>        self.beta = np.zeros((n_units, <span style=color:#ff0;font-weight:700>1</span>))
</span></span><span style=display:flex><span>        self.alpha = learning_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> forward(self, <span style=color:#fff;font-weight:700>input</span>):
</span></span><span style=display:flex><span>        self.h = np.clip(<span style=color:#fff;font-weight:700>input</span>, <span style=color:#ff0;font-weight:700>0</span>, <span style=color:#fff;font-weight:700>None</span>)
</span></span><span style=display:flex><span>        self.f = self.beta + self.omega @ self.h
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>return</span> self.f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> backward(self, dloss):
</span></span><span style=display:flex><span>        self.df_dbeta = np.eye(<span style=color:#fff;font-weight:700>len</span>(self.beta))
</span></span><span style=display:flex><span>        self.df_domega = np.copy(self.h.T)
</span></span><span style=display:flex><span>        self.df_dh = np.copy(self.omega.T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self.dloss_dbeta = self.df_dbeta @ dloss
</span></span><span style=display:flex><span>        self.dloss_domega = dloss @ self.df_domega
</span></span><span style=display:flex><span>        self.dloss_dh = self.df_dh @ dloss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self.omega -= self.alpha * self.dloss_domega
</span></span><span style=display:flex><span>        self.beta -= self.alpha * self.dloss_dbeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>class</span> InputLayer:
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> __init__(self, n_units, input_size, learning_rate):
</span></span><span style=display:flex><span>        self.omega = np.random.normal(<span style=color:#ff0;font-weight:700>0</span>, np.sqrt(<span style=color:#ff0;font-weight:700>2</span>/(n_units)), size=(n_units, input_size))
</span></span><span style=display:flex><span>        self.beta = np.zeros((n_units, <span style=color:#ff0;font-weight:700>1</span>))
</span></span><span style=display:flex><span>        self.alpha = learning_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> forward(self, <span style=color:#fff;font-weight:700>input</span>):
</span></span><span style=display:flex><span>        self.input = np.asarray(<span style=color:#fff;font-weight:700>input</span>).reshape((-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>))
</span></span><span style=display:flex><span>        self.f = self.beta + self.omega @ self.input
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>return</span> self.f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> backward(self, dloss):
</span></span><span style=display:flex><span>        self.df_dbeta = np.eye(<span style=color:#fff;font-weight:700>len</span>(self.beta))
</span></span><span style=display:flex><span>        self.df_domega = np.copy(self.input.T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self.dloss_dbeta = self.df_dbeta @ dloss
</span></span><span style=display:flex><span>        self.dloss_domega = dloss @ self.df_domega
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self.omega -= self.alpha * self.dloss_domega
</span></span><span style=display:flex><span>        self.beta -= self.alpha * self.dloss_dbeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>class</span> NN:
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> __init__(self, input_size, layers_sizes, output_size, learning_rate):
</span></span><span style=display:flex><span>        self.input_layer = InputLayer(layers_sizes[<span style=color:#ff0;font-weight:700>0</span>], input_size, learning_rate)
</span></span><span style=display:flex><span>        self.hidden_layers = []
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>for</span> l_input, l_output in <span style=color:#fff;font-weight:700>zip</span>(layers_sizes[<span style=color:#ff0;font-weight:700>0</span>:], layers_sizes[<span style=color:#ff0;font-weight:700>1</span>:]):
</span></span><span style=display:flex><span>            self.hidden_layers.append(HiddenLayer(l_output, l_input, learning_rate))
</span></span><span style=display:flex><span>        self.output_layer = HiddenLayer(output_size, layers_sizes[-<span style=color:#ff0;font-weight:700>1</span>], learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> forward(self, <span style=color:#fff;font-weight:700>input</span>):
</span></span><span style=display:flex><span>        o = self.input_layer.forward(<span style=color:#fff;font-weight:700>input</span>)
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>for</span> l in self.hidden_layers:
</span></span><span style=display:flex><span>            o = l.forward(o)
</span></span><span style=display:flex><span>        o = self.output_layer.forward(o)
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>return</span> o
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> backward(self, predicted_value, true_value):
</span></span><span style=display:flex><span>        dloss = <span style=color:#ff0;font-weight:700>2</span> * (predicted_value - true_value)
</span></span><span style=display:flex><span>        self.output_layer.backward(dloss)
</span></span><span style=display:flex><span>        dfprev_dhprev = self.output_layer.df_dh
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#fff;font-weight:700>for</span> l in self.hidden_layers[::-<span style=color:#ff0;font-weight:700>1</span>]:
</span></span><span style=display:flex><span>            dhprev_df = np.diag((l.f &gt; <span style=color:#ff0;font-weight:700>0</span>).flatten()).astype(<span style=color:#fff;font-weight:700>int</span>)
</span></span><span style=display:flex><span>            dloss = dhprev_df @ dfprev_dhprev @ dloss
</span></span><span style=display:flex><span>            l.backward(dloss)
</span></span><span style=display:flex><span>            dfprev_dhprev = l.df_dh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        dhprev_df = np.diag((self.input_layer.f &gt; <span style=color:#ff0;font-weight:700>0</span>).flatten()).astype(<span style=color:#fff;font-weight:700>int</span>)
</span></span><span style=display:flex><span>        dloss = dhprev_df @ dfprev_dhprev @ dloss
</span></span><span style=display:flex><span>        self.input_layer.backward(dloss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>def</span> train(self, <span style=color:#fff;font-weight:700>input</span>, true_value):
</span></span><span style=display:flex><span>        p = self.forward(<span style=color:#fff;font-weight:700>input</span>)
</span></span><span style=display:flex><span>        self.backward(p, true_value)
</span></span></code></pre></div></div></div><p>It&rsquo;s the time to define our first function $f(x) = \sin(10 x) + 0.75$ and train our NN to approximate it:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fff;font-weight:700>import</span> numpy <span style=color:#fff;font-weight:700>as</span> np
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>import</span> matplotlib.pylab <span style=color:#fff;font-weight:700>as</span> plt
</span></span><span style=display:flex><span>%matplotlib inline
</span></span><span style=display:flex><span>%config InlineBackend.figure_format=<span style=color:#0ff;font-weight:700>&#39;retina&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># our function that will be approximated by our NN</span>
</span></span><span style=display:flex><span>f = <span style=color:#fff;font-weight:700>lambda</span> x: np.sin(x * <span style=color:#ff0;font-weight:700>10</span>) + <span style=color:#ff0;font-weight:700>0.75</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># NN structure - 3 layers, each having 12 units, input and output are 1d</span>
</span></span><span style=display:flex><span>learning_rate = <span style=color:#ff0;font-weight:700>0.01</span>
</span></span><span style=display:flex><span>nn = NN(<span style=color:#ff0;font-weight:700>1</span>, (<span style=color:#ff0;font-weight:700>24</span>, <span style=color:#ff0;font-weight:700>24</span>, <span style=color:#ff0;font-weight:700>24</span>), <span style=color:#ff0;font-weight:700>1</span>, learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># generate 10000 samples and train NN</span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>for</span> b in <span style=color:#fff;font-weight:700>range</span>(<span style=color:#ff0;font-weight:700>0</span>, <span style=color:#ff0;font-weight:700>10000</span>):
</span></span><span style=display:flex><span>    x = np.random.uniform(-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, size = (<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>))
</span></span><span style=display:flex><span>    y = f(*x)
</span></span><span style=display:flex><span>    nn.train(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># now let&#39;s get true values and predictions for some range</span>
</span></span><span style=display:flex><span>x = np.linspace(-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>100</span>)
</span></span><span style=display:flex><span>y_predict = np.zeros_like(x)
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>for</span> i, xx in <span style=color:#fff;font-weight:700>enumerate</span>(x):
</span></span><span style=display:flex><span>    y_predict[i] = nn.forward(xx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># visualize </span>
</span></span><span style=display:flex><span>fig, ax = plt.subplots(<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, figsize = (<span style=color:#ff0;font-weight:700>6</span>, <span style=color:#ff0;font-weight:700>3</span>))
</span></span><span style=display:flex><span>ax.plot(x, f(x), label = <span style=color:#0ff;font-weight:700>&#34;True function&#34;</span>)
</span></span><span style=display:flex><span>ax.plot(x, y_predict, label = <span style=color:#0ff;font-weight:700>&#34;NN approximation&#34;</span>)
</span></span><span style=display:flex><span>ax.set(xlabel=<span style=color:#0ff;font-weight:700>&#39;$x$&#39;</span>, ylabel=<span style=color:#0ff;font-weight:700>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax.grid()
</span></span><span style=display:flex><span>ax.legend(loc=<span style=color:#0ff;font-weight:700>&#39;upper left&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_13_1.png#center alt=png></p><p>Let&rsquo;s do the same for the second function, which is 2d:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007f7f># our function that will be approximated by our NN</span>
</span></span><span style=display:flex><span>f = <span style=color:#fff;font-weight:700>lambda</span> x, y: <span style=color:#ff0;font-weight:700>25</span> * x + <span style=color:#ff0;font-weight:700>17.8</span> * y + <span style=color:#ff0;font-weight:700>0.25</span> * x * y + x * x * <span style=color:#ff0;font-weight:700>15.0</span> - y * y * <span style=color:#ff0;font-weight:700>5.0</span> - <span style=color:#ff0;font-weight:700>25</span> * (x &lt; <span style=color:#ff0;font-weight:700>0.0</span> and y &lt; <span style=color:#ff0;font-weight:700>0.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># NN structure - 2 layers, each having 12 units, input is 2d, output is 1d</span>
</span></span><span style=display:flex><span>learning_rate = <span style=color:#ff0;font-weight:700>0.001</span>
</span></span><span style=display:flex><span>nn = NN(<span style=color:#ff0;font-weight:700>2</span>, (<span style=color:#ff0;font-weight:700>12</span>, <span style=color:#ff0;font-weight:700>24</span>), <span style=color:#ff0;font-weight:700>1</span>, learning_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># generate 10000 samples and train NN</span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>for</span> b in <span style=color:#fff;font-weight:700>range</span>(<span style=color:#ff0;font-weight:700>0</span>, <span style=color:#ff0;font-weight:700>10000</span>):
</span></span><span style=display:flex><span>    x = np.random.uniform(-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, size = (<span style=color:#ff0;font-weight:700>2</span>, <span style=color:#ff0;font-weight:700>1</span>))
</span></span><span style=display:flex><span>    y = f(*x)
</span></span><span style=display:flex><span>    nn.train(x, y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># now let&#39;s get true values and predictions for some range</span>
</span></span><span style=display:flex><span>x, y = np.arange(-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>0.05</span>), np.arange(-<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>0.05</span>)
</span></span><span style=display:flex><span>y_predict = np.zeros((x.shape[<span style=color:#ff0;font-weight:700>0</span>], y.shape[<span style=color:#ff0;font-weight:700>0</span>]))
</span></span><span style=display:flex><span>y_true = np.zeros((x.shape[<span style=color:#ff0;font-weight:700>0</span>], y.shape[<span style=color:#ff0;font-weight:700>0</span>]))
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>for</span> i, xx in <span style=color:#fff;font-weight:700>enumerate</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>for</span> j, yy in <span style=color:#fff;font-weight:700>enumerate</span>(y):
</span></span><span style=display:flex><span>        y_predict[i, j] = nn.forward([xx, yy])
</span></span><span style=display:flex><span>        y_true[i, j] = f(xx, yy)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007f7f># visualize </span>
</span></span><span style=display:flex><span>fig, ax = plt.subplots(<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>2</span>, figsize = (<span style=color:#ff0;font-weight:700>6</span>, <span style=color:#ff0;font-weight:700>3</span>))
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>0</span>].imshow(y_true, extent=[x.min(), x.max(), y.min(), y.max()], origin=<span style=color:#0ff;font-weight:700>&#34;lower&#34;</span>, cmap=<span style=color:#0ff;font-weight:700>&#39;cool&#39;</span>, vmin=np.min(y_true), vmax=np.max(y_true))
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>0</span>].set(xlabel=<span style=color:#0ff;font-weight:700>&#39;$x$&#39;</span>, ylabel=<span style=color:#0ff;font-weight:700>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>0</span>].set_title(<span style=color:#0ff;font-weight:700>&#34;True function&#34;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>1</span>].imshow(y_predict, extent=[x.min(), x.max(), y.min(), y.max()], origin=<span style=color:#0ff;font-weight:700>&#34;lower&#34;</span>, cmap=<span style=color:#0ff;font-weight:700>&#39;cool&#39;</span>, vmin=np.min(y_true), vmax=np.max(y_true))
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>1</span>].set(xlabel=<span style=color:#0ff;font-weight:700>&#39;$x$&#39;</span>, ylabel=<span style=color:#0ff;font-weight:700>&#39;$y$&#39;</span>)
</span></span><span style=display:flex><span>ax[<span style=color:#ff0;font-weight:700>1</span>].set_title(<span style=color:#0ff;font-weight:700>&#34;NN approximation&#34;</span>)
</span></span><span style=display:flex><span>fig.tight_layout()
</span></span></code></pre></div></div></div><p><img src=output_15_0.png#center alt=png></p><p>Bingo!</p><p>The great video on how neural networks learn to approximate any function is <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h2 id=references>References
<a class=heading-link href=#references><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://udlbook.github.io/udlbook/ class=external-link target=_blank rel=noopener>https://udlbook.github.io/udlbook/</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu class=external-link target=_blank rel=noopener>https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4 class=external-link target=_blank rel=noopener>https://towardsdatascience.com/kaiming-he-initialization-in-neural-networks-math-proof-73b9a0d845c4</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://youtu.be/TkwXa7Cvfr8 class=external-link target=_blank rel=noopener>https://youtu.be/TkwXa7Cvfr8</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2024
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>