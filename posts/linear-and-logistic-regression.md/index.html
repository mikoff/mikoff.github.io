<!doctype html><html lang=en><head><title>Linear and logistic regressions · Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.
Spoiler heights = np.array([120, 135, 145, 150, 151, 155, 165, 170, 172, 175, 180, 190]) labels = np.array([0, 0, 0, 0., 0, 0, 1, 1, 1, 1, 1, 1.]) females = labels == 0 males = labels == 1 fig, ax = plt.subplots(1, 1, figsize = (6, 2)) ax.scatter(heights[females], labels[females]) ax.scatter(heights[males], labels[males]) ax.set(xlabel = 'height, cm', ylabel = 'class') ax."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear and logistic regressions"><meta name=twitter:description content="Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.
Spoiler heights = np.array([120, 135, 145, 150, 151, 155, 165, 170, 172, 175, 180, 190]) labels = np.array([0, 0, 0, 0., 0, 0, 1, 1, 1, 1, 1, 1.]) females = labels == 0 males = labels == 1 fig, ax = plt.subplots(1, 1, figsize = (6, 2)) ax.scatter(heights[females], labels[females]) ax.scatter(heights[males], labels[males]) ax.set(xlabel = 'height, cm', ylabel = 'class') ax."><meta property="og:title" content="Linear and logistic regressions"><meta property="og:description" content="Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.
Spoiler heights = np.array([120, 135, 145, 150, 151, 155, 165, 170, 172, 175, 180, 190]) labels = np.array([0, 0, 0, 0., 0, 0, 1, 1, 1, 1, 1, 1.]) females = labels == 0 males = labels == 1 fig, ax = plt.subplots(1, 1, figsize = (6, 2)) ax.scatter(heights[females], labels[females]) ax.scatter(heights[males], labels[males]) ax.set(xlabel = 'height, cm', ylabel = 'class') ax."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/linear-and-logistic-regression.md/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-10T19:00:00+03:00"><meta property="article:modified_time" content="2022-02-10T19:00:00+03:00"><link rel=canonical href=https://mikoff.github.io/posts/linear-and-logistic-regression.md/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li><li class=navigation-item><a class=navigation-link href=/notes/>Notes</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://mikoff.github.io/posts/linear-and-logistic-regression.md/>Linear and logistic regressions</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2022-02-10T19:00:00+03:00>February 10, 2022
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
7-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/linear-regression/>Linear Regression</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/logistic-regression/>Logistic Regression</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/sigmoid/>Sigmoid</a></span></div></div></header><div class=post-content><p>Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>heights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>120</span>, <span style=color:#099>135</span>, <span style=color:#099>145</span>, <span style=color:#099>150</span>, <span style=color:#099>151</span>, <span style=color:#099>155</span>, <span style=color:#099>165</span>, <span style=color:#099>170</span>, <span style=color:#099>172</span>, <span style=color:#099>175</span>, <span style=color:#099>180</span>, <span style=color:#099>190</span>])
</span></span><span style=display:flex><span>labels <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0.</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1.</span>])
</span></span><span style=display:flex><span>females <span style=color:#000;font-weight:700>=</span> labels <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>
</span></span><span style=display:flex><span>males <span style=color:#000;font-weight:700>=</span> labels <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;height, cm&#39;</span>, ylabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend([<span style=color:#d14>&#39;female&#39;</span>, <span style=color:#d14>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span></code></pre></div></div></div><p><img src=output_2_0.png#center alt=png></p><p>What we want to do is to find a function that approximates $p(\mathbf{Y}|\mathbf{X})$.</p><h2 id=linear-regression>Linear regression
<a class=heading-link href=#linear-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Let&rsquo;s say we are given $N$ inputs and outputs. We want to find the linear function that maps our input $x$ to our output $y$:</p>$$
y = f(x) = ax + b,
$$<p>or, in general case</p>$$
y = f(\boldsymbol{x}) = \boldsymbol{\theta}^T \boldsymbol{x},
$$<p>if our our prediction depends not on one parameter, but on $n$ of them: $\boldsymbol{x} \in \mathcal{R}^n$.</p><p>What we want to minimize is our loss function, that somehow describes the discrepancy between our predictions $\hat{y}$ and actual labels $y$ for all samples $x_i$:</p>$$
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \sum_{i=0}^{N-1}L(y_i, \hat{y}_i) = \sum_{i=0}^{N-1}L(y_i, \hat{\boldsymbol{\theta}}^T \boldsymbol{x}_i).
$$<h3 id=least-squares>Least squares
<a class=heading-link href=#least-squares><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>If we want to minimize the sum of squared residuals we end up with:</p>$$
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \sum_{i=0}^{N-1}(y_i - \hat{\boldsymbol{\theta}}^T \boldsymbol{x}_i)^2.
$$<p>If we stack all inputs (and add extra column of ones, that models the constant, or shift) and outputs, we will have two matrices $\mathbf{X}$ and $\mathbf{Y}$:</p>$$
\begin{align*}
\mathbf{X} = \begin{bmatrix} \boldsymbol{x}_0^T \\\\ \vdots \\\\ \boldsymbol{x}_{N-1}^T \end{bmatrix} & & \mathbf{Y} = \begin{bmatrix} y_0 \\\\ \vdots \\\\ y_{N-1} \end{bmatrix}
\end{align*},
$$<p>we can write our loss function as:</p>$$
\begin{align*}
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) &= ||\mathbf{Y} - \mathbf{X} \boldsymbol{\theta}||^2 \\\\
&= (\mathbf{Y} - \mathbf{X} \boldsymbol{\theta})^T(\mathbf{Y} - \mathbf{X} \boldsymbol{\theta})\\\\
&= \mathbf{Y}^T\mathbf{Y} - \mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} - \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{Y} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}\\\\
&= \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}.
\end{align*}
$$<p>During the derivation we note that since $\boldsymbol{\theta}$ is a vector we can rewrite $\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{Y}$.</p><p>Finding the derivative w.r.t. parameters:</p>$$
\begin{align*}
\frac{\partial L}{\partial {\boldsymbol {\theta }}} &= \frac{\partial(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})}{\partial{\boldsymbol {\theta }}} \\\\
&= -2\mathbf{Y}^T\mathbf{X} + 2\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X} \\\\
&= \left(2\boldsymbol{\theta}^T\mathbf{X}^T - 2\mathbf{Y}^T\right)\mathbf{X}
\end{align*}
$$<p>Then set the derivative to zero and solve for $\boldsymbol{\theta}$:</p>$$
\begin{align*}
-2\mathbf{Y}^T\mathbf{X} + 2\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X} &= \boldsymbol{0} \\\\ 
\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} &= \mathbf{X}^T\mathbf{Y}\\\\ 
\boldsymbol{\theta} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}.
\end{align*}
$$<p>By doing so and plugging the available data to $\mathbf{X}$ and $\mathbf{Y}$ we find the best fit line and perform <em>linear regression</em>.</p><h3 id=maximum-likelihood>Maximum Likelihood
<a class=heading-link href=#maximum-likelihood><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The probability of multiple independent events is known as <em>joint</em> probability. The <em>joint</em> probability can be found by multiplication of the probabilities of individual events:</p>$$
p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1}p(y_i|\boldsymbol{\theta}, x_i).
$$<p>If we assume that our noises are gaussian, we can write:</p>$$
\mathcal{L} = p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}
$$<p>This probability $\mathcal{L}$ is called <strong>likelihood</strong>, applying $\log$ we get <em>log-likelihood</em>, the sum instead of product</p>$$
\begin{align*}
\mathcal{LL} &= \sum_{i=0}^{N-1}\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}\right) \\\\
&= \sum_{i=0}^{N-1}\log\frac{1}{\sqrt{2\pi\sigma^2}} + \sum_{i=0}^{N-1}\log\left(e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}\right) \\\\
&= \log\frac{1}{\sqrt{2\pi\sigma^2}} \sum_{i=0}^{N-1}1 + \sum_{i=0}^{N-1}{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2} \\\\
&= -\frac{\log{\pi\sigma^2}}{2}{N} - \frac{\log{2}}{2}N - \frac{1}{2\sigma^2}\sum_{i=0}^{N-1}(y - \hat{y})^2.
\end{align*}
$$<p>Now if we look at $\mathcal{LL}$ as a function of only $\boldsymbol{\theta}$ and differentiate $\mathcal{LL}$ with respect to $\boldsymbol{\theta}$ we end up with exact same expression as for least-squares case. That is the reason why least squares estimate can be viewed as MLE under Gaussian noise model:</p>$$
y = y_{true} + \epsilon, \quad\text{ where }\epsilon\thicksim N(0,\sigma^2).
$$<h3 id=fitting-line-to-our-data>Fitting line to our data
<a class=heading-link href=#fitting-line-to-our-data><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># linear regression</span>
</span></span><span style=display:flex><span>X <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>hstack((heights<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)), np<span style=color:#000;font-weight:700>.</span>ones((<span style=color:#0086b3>len</span>(heights), <span style=color:#099>1</span>))))
</span></span><span style=display:flex><span>Y <span style=color:#000;font-weight:700>=</span> labels<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>theta <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linalg<span style=color:#000;font-weight:700>.</span>inv(X<span style=color:#000;font-weight:700>.</span>T <span style=color:#000;font-weight:700>@</span> X) <span style=color:#000;font-weight:700>@</span> X<span style=color:#000;font-weight:700>.</span>T <span style=color:#000;font-weight:700>@</span> Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>arange(np<span style=color:#000;font-weight:700>.</span>min(heights), np<span style=color:#000;font-weight:700>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(xrange, theta[<span style=color:#099>0</span>][<span style=color:#099>0</span>] <span style=color:#000;font-weight:700>*</span> xrange <span style=color:#000;font-weight:700>+</span> theta[<span style=color:#099>1</span>][<span style=color:#099>0</span>])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;height, cm&#39;</span>, ylabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend([<span style=color:#d14>&#39;female&#39;</span>, <span style=color:#d14>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;theta: </span><span style=color:#d14>{</span><span style=color:#000;font-weight:700>*</span>theta<span style=color:#000;font-weight:700>.</span>flatten(),<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div></div></div><pre><code>theta: (0.022082018927444665, -3.011041009463699)
</code></pre><p><img src=output_12_1.png#center alt=png></p><p>Now if we want to predict the gender of a person by its height we just multiply our feature vector with $\boldsymbol{\theta}$ and obtain the result: if it is greater than 0.5 then it is a male, if lower - female. Done! But if look at this problem more closely we fill figure that sometimes the &ldquo;probability&rdquo; can be negative, or bigger than one. It is counter-intuitive, since we want to stick to probability space.
Another problem is that linear regression is sensitive to imbalances in data, for example:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>heights <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>120</span>, <span style=color:#099>121</span>, <span style=color:#099>125</span>, <span style=color:#099>135</span>, <span style=color:#099>160</span>, <span style=color:#099>161</span>, <span style=color:#099>162</span>, <span style=color:#099>163</span>, <span style=color:#099>164</span>, <span style=color:#099>165</span>, <span style=color:#099>166</span>, <span style=color:#099>167</span>, <span style=color:#099>168</span>, <span style=color:#099>170</span>, <span style=color:#099>172</span>, <span style=color:#099>175</span>, <span style=color:#099>180</span>])
</span></span><span style=display:flex><span>labels <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0.</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>0</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>, <span style=color:#099>1</span>])
</span></span><span style=display:flex><span>females <span style=color:#000;font-weight:700>=</span> labels <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>
</span></span><span style=display:flex><span>males <span style=color:#000;font-weight:700>=</span> labels <span style=color:#000;font-weight:700>==</span> <span style=color:#099>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>hstack((heights<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)), np<span style=color:#000;font-weight:700>.</span>ones((<span style=color:#0086b3>len</span>(heights), <span style=color:#099>1</span>))))
</span></span><span style=display:flex><span>Y <span style=color:#000;font-weight:700>=</span> labels<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>theta <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linalg<span style=color:#000;font-weight:700>.</span>inv(X<span style=color:#000;font-weight:700>.</span>T <span style=color:#000;font-weight:700>@</span> X) <span style=color:#000;font-weight:700>@</span> X<span style=color:#000;font-weight:700>.</span>T <span style=color:#000;font-weight:700>@</span> Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>arange(np<span style=color:#000;font-weight:700>.</span>min(heights), np<span style=color:#000;font-weight:700>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(xrange, theta[<span style=color:#099>0</span>][<span style=color:#099>0</span>] <span style=color:#000;font-weight:700>*</span> xrange <span style=color:#000;font-weight:700>+</span> theta[<span style=color:#099>1</span>][<span style=color:#099>0</span>])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;height, cm&#39;</span>, ylabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend([<span style=color:#d14>&#39;female&#39;</span>, <span style=color:#d14>&#39;male&#39;</span>], loc <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;upper left&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;theta: </span><span style=color:#d14>{</span><span style=color:#000;font-weight:700>*</span>theta<span style=color:#000;font-weight:700>.</span>flatten(),<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div></div></div><pre><code>theta: (0.013266157882184739, -1.7925709515859998)
</code></pre><p><img src=output_14_1.png#center alt=png></p><p>Now we can see that the classification is clearly wrong for males under 175cm.</p><h2 id=logistic-regression>Logistic regression
<a class=heading-link href=#logistic-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Can we find a better function that approximates our data? Let&rsquo;s have a look at the sigmoid function and plot its graph.</p>$$
\sigma(z) = \frac{1}{1 + e^{-z}} \in [0, 1].
$$<div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>sigmoid</span>(z):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> <span style=color:#099>1</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>1.</span> <span style=color:#000;font-weight:700>+</span> np<span style=color:#000;font-weight:700>.</span>exp(<span style=color:#000;font-weight:700>-</span>z))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>inputs <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(<span style=color:#000;font-weight:700>-</span><span style=color:#099>10</span>, <span style=color:#099>10</span>, <span style=color:#099>100</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(inputs, sigmoid(inputs))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;x&#39;</span>, ylabel<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;$\sigma(x)$&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_18_1.png#center alt=png></p><p>The sigmoid takes an input $z \in \mathcal{R}$ and maps it into the range $(0, 1)$. Since we have only two classes we can use Bernoulli distribution.
The likelihood of the data can be written as:</p>$$
\mathcal{L} = p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1} \sigma\left(\hat{y}\right)^y \left(1 - \sigma\left(\hat{y}\right)\right)^{1-y},~~~\hat{y}=\boldsymbol{\theta}^T\boldsymbol{x}.
$$<p>Taking the logarithm, we obtain:</p>$$
\mathcal{LL} = -\sum_{i=0}^{N-1}\left(y\log\sigma\left(\hat{y}\right) + \left(1 - y\right)\log(1 - \sigma\left(\hat{y}\right) \right)
$$<p>That is the function we want to minimize. Let&rsquo;s look at the penalties that generates the aforementioned log-likelihood. To do so we plot two graphs: one for the case when the true label is $0$ and the other for the case when the true label is $1$. If our prediction is close to the correct label the penalization term approaches zero, if the prediction is completely wrong the penalization term has very high value. That is desirable behaviour for the classification problem.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>ll</span>(label, prediction):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> <span style=color:#000;font-weight:700>-</span> label <span style=color:#000;font-weight:700>*</span> np<span style=color:#000;font-weight:700>.</span>log(prediction) <span style=color:#000;font-weight:700>-</span> (<span style=color:#099>1</span> <span style=color:#000;font-weight:700>-</span> label) <span style=color:#000;font-weight:700>*</span> np<span style=color:#000;font-weight:700>.</span>log(<span style=color:#099>1</span> <span style=color:#000;font-weight:700>-</span> prediction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(sigmoid(inputs), ll(<span style=color:#099>1</span>, sigmoid(inputs)), label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$y = 1$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(sigmoid(inputs), ll(<span style=color:#099>0</span>, sigmoid(inputs)), label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$y = 0$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;Probability&#39;</span>, ylabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;Penalization term&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_22_1.png#center alt=png></p><p>Writing the log likelihood $\mathcal{LL}$ in matrix notation:</p>$$
\begin{align*}
\mathcal{LL} = -\mathbf{Y}^T\log\sigma\left( \mathbf{X}\boldsymbol{\theta} \right) - (\mathbf{1}^T - \mathbf{Y}^T)\log\left(\mathbf{1} - \sigma\left( \mathbf{X}\boldsymbol{\theta} \right) \right) \\\\
\end{align*}
$$<p>The derivative w.r.t. $\boldsymbol{\theta}$ can be found to be <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p>$$
\frac{\partial\mathcal{LL}}{\partial\boldsymbol{\theta}} = \frac{1}{N}\mathbf{X}^T\left( \sigma(\mathbf{x}\boldsymbol{\theta}) - \mathbf{Y} \right)
$$<p>Since there is no closed form solution we have to use one of the numerical optimization techniques. Let&rsquo;s use the gradient descent. I also tried the Newton method, but wasn&rsquo;t able to produce robust results for all tested cases.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>logistic_regression_gradient_descent</span>(
</span></span><span style=display:flex><span>    X, Y, step_size <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1e-4</span>, iters <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1000000</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    theta <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((X<span style=color:#000;font-weight:700>.</span>shape[<span style=color:#099>1</span>], <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(iters):
</span></span><span style=display:flex><span>        theta <span style=color:#000;font-weight:700>=</span> theta <span style=color:#000;font-weight:700>-</span>  step <span style=color:#000;font-weight:700>*</span> X<span style=color:#000;font-weight:700>.</span>T <span style=color:#000;font-weight:700>@</span> (sigmoid(X <span style=color:#000;font-weight:700>@</span> theta) <span style=color:#000;font-weight:700>-</span> Y)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> theta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Y <span style=color:#000;font-weight:700>=</span> labels<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>X <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>hstack((heights<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)), np<span style=color:#000;font-weight:700>.</span>ones((<span style=color:#0086b3>len</span>(Y), <span style=color:#099>1</span>))))
</span></span><span style=display:flex><span>theta <span style=color:#000;font-weight:700>=</span> logistic_regression_gradient_descent(X, Y, step_size <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1e-2</span>)
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>f</span><span style=color:#d14>&#39;theta: </span><span style=color:#d14>{</span><span style=color:#000;font-weight:700>*</span>theta<span style=color:#000;font-weight:700>.</span>flatten(),<span style=color:#d14>}</span><span style=color:#d14>&#39;</span>)
</span></span></code></pre></div><pre><code>theta: (17.057175819704746, -2857.0739642526546)
</code></pre><p>And let&rsquo;s look how our function approximates the probability of a point to be related to one or another class:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>inputs <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>arange(<span style=color:#099>100</span>, <span style=color:#099>200</span>)
</span></span><span style=display:flex><span>outputs <span style=color:#000;font-weight:700>=</span> sigmoid(np<span style=color:#000;font-weight:700>.</span>hstack((inputs<span style=color:#000;font-weight:700>.</span>reshape((<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>)), np<span style=color:#000;font-weight:700>.</span>ones((<span style=color:#0086b3>len</span>(inputs), <span style=color:#099>1</span>)))) <span style=color:#000;font-weight:700>@</span> theta)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>6</span>, <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>arange(np<span style=color:#000;font-weight:700>.</span>min(heights), np<span style=color:#000;font-weight:700>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(inputs, outputs)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set(xlabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;height, cm&#39;</span>, ylabel <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend([<span style=color:#d14>&#39;female&#39;</span>, <span style=color:#d14>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>grid()
</span></span></code></pre></div></div></div><p><img src=output_27_1.png#center alt=png></p><p>Much better now!</p><h2 id=logarithm-roots>Logarithm roots
<a class=heading-link href=#logarithm-roots><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The key thing in logistic regression is the sequence of operation:</p><ol><li>converting the probability to odds ratio (the range $[0, 1]$ of the distribution function of a PDF is being mapped to range $(0, \infty)$;</li><li>converting odds to log-odds (maps $(0, \infty)$ range to $(-\infty, \infty)$;</li><li>the last step allows us to use regression on unconstrained space:
$$
\log \frac{p_i}{1-p_i} = \boldsymbol{\theta}^T\mathbf{x_i}
$$</li></ol><p>After these transforms we have a linear model for the logistic regression.</p><p>Of course, we can go vice versa now and get the sigmoid function that we use during the optimization and mapping between domains:</p>$$
\begin{align*}
\log \frac{p}{1-p} &= \boldsymbol{\theta}^T\mathbf{x} \\\\
\frac{p_i}{1-p} &= e^{~\boldsymbol{\theta}^T\mathbf{x}} \\\\
p &= \frac{e^{~\boldsymbol{\theta}^T\mathbf{x}}}{1 + e^{~\boldsymbol{\theta}^T\mathbf{x}}} \\\\
p &= \frac{1}{1 + e^{-\boldsymbol{\theta}^T\mathbf{x}}}
\end{align*}
$$<h2 id=references>References
<a class=heading-link href=#references><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf class=external-link target=_blank rel=noopener>https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2024
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>