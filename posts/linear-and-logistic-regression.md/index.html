<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Aleksandr Mikoff"><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear and logistic regressions"><meta name=twitter:description content="Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.
Spoiler heights = np.array([120, 135, 145, 150, 151, 155, 165, 170, 172, 175, 180, 190]) labels = np.array([0, 0, 0, 0., 0, 0, 1, 1, 1, 1, 1, 1.]) females = labels == 0 males = labels == 1 fig, ax = plt.subplots(1, 1, figsize = (6, 2)) ax.scatter(heights[females], labels[females]) ax.scatter(heights[males], labels[males]) ax.set(xlabel = 'height, cm', ylabel = 'class') ax."><meta property="og:title" content="Linear and logistic regressions"><meta property="og:description" content="Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.
Spoiler heights = np.array([120, 135, 145, 150, 151, 155, 165, 170, 172, 175, 180, 190]) labels = np.array([0, 0, 0, 0., 0, 0, 1, 1, 1, 1, 1, 1.]) females = labels == 0 males = labels == 1 fig, ax = plt.subplots(1, 1, figsize = (6, 2)) ax.scatter(heights[females], labels[females]) ax.scatter(heights[males], labels[males]) ax.set(xlabel = 'height, cm', ylabel = 'class') ax."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/linear-and-logistic-regression.md/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-10T19:00:00+03:00"><meta property="article:modified_time" content="2022-02-10T19:00:00+03:00"><base href=https://mikoff.github.io/posts/linear-and-logistic-regression.md/><title>Linear and logistic regressions · Aleksandr Mikoff's blog</title><link rel=canonical href=https://mikoff.github.io/posts/linear-and-logistic-regression.md/><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.11.2/css/all.css integrity=sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin=anonymous><link rel=stylesheet href=https://mikoff.github.io/fontdata/css/academicons.min.css><link rel=stylesheet href=https://mikoff.github.io/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/coder-dark.min.83a2010dac9f59f943b3004cd6c4f230507ad036da635d3621401d42ec4e2835.css integrity="sha256-g6IBDayfWflDswBM1sTyMFB60DbaY102IUAdQuxOKDU=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://mikoff.github.io/css/image.css><link rel=stylesheet href=https://mikoff.github.io/css/spoiler.css><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://mikoff.github.io/img/favicon-16x16.png sizes=16x16><meta name=generator content="Hugo 0.119.0"></head><body class=colorscheme-auto><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fas fa-bars"></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=https://mikoff.github.io/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title>Linear and logistic regressions</h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fas fa-calendar"></i>
<time datetime=2022-02-10T19:00:00+03:00>February 10, 2022</time></span>
<span class=reading-time><i class="fas fa-clock"></i>
7-minute read</span></div><div class=tags><i class="fas fa-tag"></i>
<a href=https://mikoff.github.io/tags/linear-regression/>Linear regression</a>
<span class=separator>•</span>
<a href=https://mikoff.github.io/tags/logistic-regression/>Logistic regression</a>
<span class=separator>•</span>
<a href=https://mikoff.github.io/tags/sigmoid/>Sigmoid</a></div></div></header><div><p>Let&rsquo;s assume we want to predict whether the person is male or female judging by its height.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>heights <span style=color:#555>=</span> np<span style=color:#555>.</span>array([<span style=color:#f60>120</span>, <span style=color:#f60>135</span>, <span style=color:#f60>145</span>, <span style=color:#f60>150</span>, <span style=color:#f60>151</span>, <span style=color:#f60>155</span>, <span style=color:#f60>165</span>, <span style=color:#f60>170</span>, <span style=color:#f60>172</span>, <span style=color:#f60>175</span>, <span style=color:#f60>180</span>, <span style=color:#f60>190</span>])
</span></span><span style=display:flex><span>labels <span style=color:#555>=</span> np<span style=color:#555>.</span>array([<span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0.</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1.</span>])
</span></span><span style=display:flex><span>females <span style=color:#555>=</span> labels <span style=color:#555>==</span> <span style=color:#f60>0</span>
</span></span><span style=display:flex><span>males <span style=color:#555>=</span> labels <span style=color:#555>==</span> <span style=color:#f60>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel <span style=color:#555>=</span> <span style=color:#c30>&#39;height, cm&#39;</span>, ylabel <span style=color:#555>=</span> <span style=color:#c30>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend([<span style=color:#c30>&#39;female&#39;</span>, <span style=color:#c30>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span></code></pre></div></div></div><p><img src=output_2_0.png#center alt=png></p><p>What we want to do is to find a function that approximates $p(\mathbf{Y}|\mathbf{X})$.</p><h2 id=linear-regression>Linear regression</h2><p>Let&rsquo;s say we are given $N$ inputs and outputs. We want to find the linear function that maps our input $x$ to our output $y$:
$$
y = f(x) = ax + b,
$$
or, in general case
$$
y = f(\boldsymbol{x}) = \boldsymbol{\theta}^T \boldsymbol{x},
$$
if our our prediction depends not on one parameter, but on $n$ of them: $\boldsymbol{x} \in \mathcal{R}^n$.</p><p>What we want to minimize is our loss function, that somehow describes the discrepancy between our predictions $\hat{y}$ and actual labels $y$ for all samples $x_i$:
$$
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \sum_{i=0}^{N-1}L(y_i, \hat{y}_i) = \sum_{i=0}^{N-1}L(y_i, \hat{\boldsymbol{\theta}}^T \boldsymbol{x}_i).
$$</p><h3 id=least-squares>Least squares</h3><p>If we want to minimize the sum of squared residuals we end up with:
$$
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) = \sum_{i=0}^{N-1}(y_i - \hat{\boldsymbol{\theta}}^T \boldsymbol{x}_i)^2.
$$</p><p>If we stack all inputs (and add extra column of ones, that models the constant, or shift) and outputs, we will have two matrices $\mathbf{X}$ and $\mathbf{Y}$:
$$
\begin{align*}
\mathbf{X} = \begin{bmatrix} \boldsymbol{x}_0^T \\ \vdots \\ \boldsymbol{x}_{N-1}^T \end{bmatrix} & & \mathbf{Y} = \begin{bmatrix} y_0 \\ \vdots \\ y_{N-1} \end{bmatrix}
\end{align*},
$$
we can write our loss function as:
$$
\begin{align*}
L(\boldsymbol{y}, \hat{\boldsymbol{y}}) &= ||\mathbf{Y} - \mathbf{X} \boldsymbol{\theta}||^2 \\
&= (\mathbf{Y} - \mathbf{X} \boldsymbol{\theta})^T(\mathbf{Y} - \mathbf{X} \boldsymbol{\theta})\\
&= \mathbf{Y}^T\mathbf{Y} - \mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} - \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{Y} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}\\
&= \mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}.
\end{align*}
$$
During the derivation we note that since $\boldsymbol{\theta}$ is a vector we can rewrite $\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta}$ as $\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{Y}$.</p><p>Finding the derivative w.r.t. parameters:
$$
\begin{align*}
\frac{\partial L}{\partial {\boldsymbol {\theta }}} &= \frac{\partial(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{X}\boldsymbol{\theta} + \boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\theta})}{\partial{\boldsymbol {\theta }}} \\
&= -2\mathbf{Y}^T\mathbf{X} + 2\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X} \\
&= \left(2\boldsymbol{\theta}^T\mathbf{X}^T - 2\mathbf{Y}^T\right)\mathbf{X}
\end{align*}
$$
Then set the derivative to zero and solve for $\boldsymbol{\theta}$:
$$
\begin{align*}
-2\mathbf{Y}^T\mathbf{X} + 2\boldsymbol{\theta}^T\mathbf{X}^T\mathbf{X} &= \boldsymbol{0} \\
\mathbf{X}^T\mathbf{X}\boldsymbol{\theta} &= \mathbf{X}^T\mathbf{Y}\\
\boldsymbol{\theta} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}.
\end{align*}
$$
By doing so and plugging the available data to $\mathbf{X}$ and $\mathbf{Y}$ we find the best fit line and perform <em>linear regression</em>.</p><h3 id=maximum-likelihood>Maximum Likelihood</h3><p>The probability of multiple independent events is known as <em>joint</em> probability. The <em>joint</em> probability can be found by multiplication of the probabilities of individual events:
$$
p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1}p(y_i|\boldsymbol{\theta}, x_i).
$$
If we assume that our noises are gaussian, we can write:
$$
\mathcal{L} = p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}
$$</p><p>This probability $\mathcal{L}$ is called <strong>likelihood</strong>, applying $\log$ we get <em>log-likelihood</em>, the sum instead of product</p><p>$$
\begin{align*}
\mathcal{LL} &= \sum_{i=0}^{N-1}\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}\right) \\
&= \sum_{i=0}^{N-1}\log\frac{1}{\sqrt{2\pi\sigma^2}} + \sum_{i=0}^{N-1}\log\left(e^{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2}\right) \\
&= \log\frac{1}{\sqrt{2\pi\sigma^2}} \sum_{i=0}^{N-1}1 + \sum_{i=0}^{N-1}{-\frac{1}{2}\left( \frac{y - \hat{y}}{\sigma} \right)^2} \\
&= -\frac{\log{\pi\sigma^2}}{2}{N} - \frac{\log{2}}{2}N - \frac{1}{2\sigma^2}\sum_{i=0}^{N-1}(y - \hat{y})^2.
\end{align*}
$$
Now if we look at $\mathcal{LL}$ as a function of only $\boldsymbol{\theta}$ and differentiate $\mathcal{LL}$ with respect to $\boldsymbol{\theta}$ we end up with exact same expression as for least-squares case. That is the reason why least squares estimate can be viewed as MLE under Gaussian noise model:
$$
y = y_{true} + \epsilon, \quad\text{ where }\epsilon\thicksim N(0,\sigma^2).
$$</p><h3 id=fitting-line-to-our-data>Fitting line to our data</h3><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#09f;font-style:italic># linear regression</span>
</span></span><span style=display:flex><span>X <span style=color:#555>=</span> np<span style=color:#555>.</span>hstack((heights<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>)), np<span style=color:#555>.</span>ones((<span style=color:#366>len</span>(heights), <span style=color:#f60>1</span>))))
</span></span><span style=display:flex><span>Y <span style=color:#555>=</span> labels<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>theta <span style=color:#555>=</span> np<span style=color:#555>.</span>linalg<span style=color:#555>.</span>inv(X<span style=color:#555>.</span>T <span style=color:#555>@</span> X) <span style=color:#555>@</span> X<span style=color:#555>.</span>T <span style=color:#555>@</span> Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#555>=</span> np<span style=color:#555>.</span>arange(np<span style=color:#555>.</span>min(heights), np<span style=color:#555>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(xrange, theta[<span style=color:#f60>0</span>][<span style=color:#f60>0</span>] <span style=color:#555>*</span> xrange <span style=color:#555>+</span> theta[<span style=color:#f60>1</span>][<span style=color:#f60>0</span>])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel <span style=color:#555>=</span> <span style=color:#c30>&#39;height, cm&#39;</span>, ylabel <span style=color:#555>=</span> <span style=color:#c30>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend([<span style=color:#c30>&#39;female&#39;</span>, <span style=color:#c30>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;theta: </span><span style=color:#a00>{</span><span style=color:#555>*</span>theta<span style=color:#555>.</span>flatten(),<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div></div></div><pre><code>theta: (0.022082018927444665, -3.011041009463699)
</code></pre><p><img src=output_12_1.png#center alt=png></p><p>Now if we want to predict the gender of a person by its height we just multiply our feature vector with $\boldsymbol{\theta}$ and obtain the result: if it is greater than 0.5 then it is a male, if lower - female. Done! But if look at this problem more closely we fill figure that sometimes the &ldquo;probability&rdquo; can be negative, or bigger than one. It is counter-intuitive, since we want to stick to probability space.
Another problem is that linear regression is sensitive to imbalances in data, for example:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>heights <span style=color:#555>=</span> np<span style=color:#555>.</span>array([<span style=color:#f60>120</span>, <span style=color:#f60>121</span>, <span style=color:#f60>125</span>, <span style=color:#f60>135</span>, <span style=color:#f60>160</span>, <span style=color:#f60>161</span>, <span style=color:#f60>162</span>, <span style=color:#f60>163</span>, <span style=color:#f60>164</span>, <span style=color:#f60>165</span>, <span style=color:#f60>166</span>, <span style=color:#f60>167</span>, <span style=color:#f60>168</span>, <span style=color:#f60>170</span>, <span style=color:#f60>172</span>, <span style=color:#f60>175</span>, <span style=color:#f60>180</span>])
</span></span><span style=display:flex><span>labels <span style=color:#555>=</span> np<span style=color:#555>.</span>array([<span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0.</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>0</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>, <span style=color:#f60>1</span>])
</span></span><span style=display:flex><span>females <span style=color:#555>=</span> labels <span style=color:#555>==</span> <span style=color:#f60>0</span>
</span></span><span style=display:flex><span>males <span style=color:#555>=</span> labels <span style=color:#555>==</span> <span style=color:#f60>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#555>=</span> np<span style=color:#555>.</span>hstack((heights<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>)), np<span style=color:#555>.</span>ones((<span style=color:#366>len</span>(heights), <span style=color:#f60>1</span>))))
</span></span><span style=display:flex><span>Y <span style=color:#555>=</span> labels<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>theta <span style=color:#555>=</span> np<span style=color:#555>.</span>linalg<span style=color:#555>.</span>inv(X<span style=color:#555>.</span>T <span style=color:#555>@</span> X) <span style=color:#555>@</span> X<span style=color:#555>.</span>T <span style=color:#555>@</span> Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#555>=</span> np<span style=color:#555>.</span>arange(np<span style=color:#555>.</span>min(heights), np<span style=color:#555>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(xrange, theta[<span style=color:#f60>0</span>][<span style=color:#f60>0</span>] <span style=color:#555>*</span> xrange <span style=color:#555>+</span> theta[<span style=color:#f60>1</span>][<span style=color:#f60>0</span>])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel <span style=color:#555>=</span> <span style=color:#c30>&#39;height, cm&#39;</span>, ylabel <span style=color:#555>=</span> <span style=color:#c30>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend([<span style=color:#c30>&#39;female&#39;</span>, <span style=color:#c30>&#39;male&#39;</span>], loc <span style=color:#555>=</span> <span style=color:#c30>&#39;upper left&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;theta: </span><span style=color:#a00>{</span><span style=color:#555>*</span>theta<span style=color:#555>.</span>flatten(),<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div></div></div><pre><code>theta: (0.013266157882184739, -1.7925709515859998)
</code></pre><p><img src=output_14_1.png#center alt=png></p><p>Now we can see that the classification is clearly wrong for males under 175cm.</p><h2 id=logistic-regression>Logistic regression</h2><p>Can we find a better function that approximates our data? Let&rsquo;s have a look at the sigmoid function and plot its graph.
$$
\sigma(z) = \frac{1}{1 + e^{-z}} \in [0, 1].
$$</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>sigmoid</span>(z):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>return</span> <span style=color:#f60>1</span> <span style=color:#555>/</span> (<span style=color:#f60>1.</span> <span style=color:#555>+</span> np<span style=color:#555>.</span>exp(<span style=color:#555>-</span>z))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>inputs <span style=color:#555>=</span> np<span style=color:#555>.</span>linspace(<span style=color:#555>-</span><span style=color:#f60>10</span>, <span style=color:#f60>10</span>, <span style=color:#f60>100</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(inputs, sigmoid(inputs))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel<span style=color:#555>=</span><span style=color:#c30>&#39;x&#39;</span>, ylabel<span style=color:#555>=</span><span style=color:#c30>&#39;$\sigma(x)$&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_18_1.png#center alt=png></p><p>The sigmoid takes an input $z \in \mathcal{R}$ and maps it into the range $(0, 1)$. Since we have only two classes we can use Bernoulli distribution.
The likelihood of the data can be written as:
$$
\mathcal{L} = p(\mathbf{Y}| \boldsymbol{\theta}, \mathbf{X}) = \prod_{i=0}^{N-1} \sigma\left(\hat{y}\right)^y \left(1 - \sigma\left(\hat{y}\right)\right)^{1-y},~~~\hat{y}=\boldsymbol{\theta}^T\boldsymbol{x}.
$$</p><p>Taking the logarithm, we obtain:
$$
\mathcal{LL} = -\sum_{i=0}^{N-1}\left(y\log\sigma\left(\hat{y}\right) + \left(1 - y\right)\log(1 - \sigma\left(\hat{y}\right) \right)
$$</p><p>That is the function we want to minimize. Let&rsquo;s look at the penalties that generates the aforementioned log-likelihood. To do so we plot two graphs: one for the case when the true label is $0$ and the other for the case when the true label is $1$. If our prediction is close to the correct label the penalization term approaches zero, if the prediction is completely wrong the penalization term has very high value. That is desirable behaviour for the classification problem.</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>ll</span>(label, prediction):
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>return</span> <span style=color:#555>-</span> label <span style=color:#555>*</span> np<span style=color:#555>.</span>log(prediction) <span style=color:#555>-</span> (<span style=color:#f60>1</span> <span style=color:#555>-</span> label) <span style=color:#555>*</span> np<span style=color:#555>.</span>log(<span style=color:#f60>1</span> <span style=color:#555>-</span> prediction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(sigmoid(inputs), ll(<span style=color:#f60>1</span>, sigmoid(inputs)), label <span style=color:#555>=</span> <span style=color:#c30>&#39;$y = 1$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(sigmoid(inputs), ll(<span style=color:#f60>0</span>, sigmoid(inputs)), label <span style=color:#555>=</span> <span style=color:#c30>&#39;$y = 0$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend()
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel <span style=color:#555>=</span> <span style=color:#c30>&#39;Probability&#39;</span>, ylabel <span style=color:#555>=</span> <span style=color:#c30>&#39;Penalization term&#39;</span>)
</span></span></code></pre></div></div></div><p><img src=output_22_1.png#center alt=png></p><p>Writing the log likelihood $\mathcal{LL}$ in matrix notation:
$$
\begin{align*}
\mathcal{LL} = -\mathbf{Y}^T\log\sigma\left( \mathbf{X}\boldsymbol{\theta} \right) - (\mathbf{1}^T - \mathbf{Y}^T)\log\left(\mathbf{1} - \sigma\left( \mathbf{X}\boldsymbol{\theta} \right) \right) \\
\end{align*}
$$
The derivative w.r.t. $\boldsymbol{\theta}$ can be found to be <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:
$$
\frac{\partial\mathcal{LL}}{\partial\boldsymbol{\theta}} = \frac{1}{N}\mathbf{X}^T\left( \sigma(\mathbf{x}\boldsymbol{\theta}) - \mathbf{Y} \right)
$$</p><p>Since there is no closed form solution we have to use one of the numerical optimization techniques. Let&rsquo;s use the gradient descent. I also tried the Newton method, but wasn&rsquo;t able to produce robust results for all tested cases.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#069;font-weight:700>def</span> <span style=color:#c0f>logistic_regression_gradient_descent</span>(
</span></span><span style=display:flex><span>    X, Y, step_size <span style=color:#555>=</span> <span style=color:#f60>1e-4</span>, iters <span style=color:#555>=</span> <span style=color:#f60>1000000</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    theta <span style=color:#555>=</span> np<span style=color:#555>.</span>zeros((X<span style=color:#555>.</span>shape[<span style=color:#f60>1</span>], <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#366>range</span>(iters):
</span></span><span style=display:flex><span>        theta <span style=color:#555>=</span> theta <span style=color:#555>-</span>  step <span style=color:#555>*</span> X<span style=color:#555>.</span>T <span style=color:#555>@</span> (sigmoid(X <span style=color:#555>@</span> theta) <span style=color:#555>-</span> Y)
</span></span><span style=display:flex><span>    <span style=color:#069;font-weight:700>return</span> theta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Y <span style=color:#555>=</span> labels<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>))
</span></span><span style=display:flex><span>X <span style=color:#555>=</span> np<span style=color:#555>.</span>hstack((heights<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>)), np<span style=color:#555>.</span>ones((<span style=color:#366>len</span>(Y), <span style=color:#f60>1</span>))))
</span></span><span style=display:flex><span>theta <span style=color:#555>=</span> logistic_regression_gradient_descent(X, Y, step_size <span style=color:#555>=</span> <span style=color:#f60>1e-2</span>)
</span></span><span style=display:flex><span><span style=color:#366>print</span>(<span style=color:#c30>f</span><span style=color:#c30>&#39;theta: </span><span style=color:#a00>{</span><span style=color:#555>*</span>theta<span style=color:#555>.</span>flatten(),<span style=color:#a00>}</span><span style=color:#c30>&#39;</span>)
</span></span></code></pre></div><pre><code>theta: (17.057175819704746, -2857.0739642526546)
</code></pre><p>And let&rsquo;s look how our function approximates the probability of a point to be related to one or another class:</p><div class=spoiler><span class=spoilerText>Spoiler</span>
<input class=spoilerChecked type=checkbox showtext=Code><div class=spoilerContent><div class=highlight><pre tabindex=0 style=background-color:#f0f3f3;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>inputs <span style=color:#555>=</span> np<span style=color:#555>.</span>arange(<span style=color:#f60>100</span>, <span style=color:#f60>200</span>)
</span></span><span style=display:flex><span>outputs <span style=color:#555>=</span> sigmoid(np<span style=color:#555>.</span>hstack((inputs<span style=color:#555>.</span>reshape((<span style=color:#555>-</span><span style=color:#f60>1</span>, <span style=color:#f60>1</span>)), np<span style=color:#555>.</span>ones((<span style=color:#366>len</span>(inputs), <span style=color:#f60>1</span>)))) <span style=color:#555>@</span> theta)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#555>=</span> plt<span style=color:#555>.</span>subplots(<span style=color:#f60>1</span>, <span style=color:#f60>1</span>, figsize <span style=color:#555>=</span> (<span style=color:#f60>6</span>, <span style=color:#f60>2</span>))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[females], labels[females])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>scatter(heights[males], labels[males])
</span></span><span style=display:flex><span>xrange <span style=color:#555>=</span> np<span style=color:#555>.</span>arange(np<span style=color:#555>.</span>min(heights), np<span style=color:#555>.</span>max(heights))
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>plot(inputs, outputs)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>set(xlabel <span style=color:#555>=</span> <span style=color:#c30>&#39;height, cm&#39;</span>, ylabel <span style=color:#555>=</span> <span style=color:#c30>&#39;class&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>legend([<span style=color:#c30>&#39;female&#39;</span>, <span style=color:#c30>&#39;male&#39;</span>])
</span></span><span style=display:flex><span>ax<span style=color:#555>.</span>grid()
</span></span></code></pre></div></div></div><p><img src=output_27_1.png#center alt=png></p><p>Much better now!</p><h2 id=logarithm-roots>Logarithm roots</h2><p>The key thing in logistic regression is the sequence of operation:</p><ol><li>converting the probability to odds ratio (the range $[0, 1]$ of the distribution function of a PDF is being mapped to range $(0, \infty)$;</li><li>converting odds to log-odds (maps $(0, \infty)$ range to $(-\infty, \infty)$;</li><li>the last step allows us to use regression on unconstrained space:
$$
\log \frac{p_i}{1-p_i} = \boldsymbol{\theta}^T\mathbf{x_i}
$$</li></ol><p>After these transforms we have a linear model for the logistic regression.</p><p>Of course, we can go vice versa now and get the sigmoid function that we use during the optimization and mapping between domains:
$$
\begin{align*}
\log \frac{p}{1-p} &= \boldsymbol{\theta}^T\mathbf{x} \\
\frac{p_i}{1-p} &= e^{~\boldsymbol{\theta}^T\mathbf{x}} \\
p &= \frac{e^{~\boldsymbol{\theta}^T\mathbf{x}}}{1 + e^{~\boldsymbol{\theta}^T\mathbf{x}}} \\
p &= \frac{1}{1 + e^{-\boldsymbol{\theta}^T\mathbf{x}}}
\end{align*}
$$</p><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>© 2023
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.slim.min.js></script></main></body></html>