<!doctype html><html lang=en><head><title>Likelihood and probability normalization, log-sum-exp trick · Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Working with probabilities involves multiplication and normalization of their values. Since the numerical values sometimes are extremely low that can lead to underflow problems. This problem is evident with particle filters - we have to multiply really low likelihood values that vanish in the end. Log-sum-exp allows to abbreviate this problem.
Approach Link to heading Log-likelihoods Link to heading Since the likelihood values can be extremely low it is more convenient to work with loglikelihood instead of likelihood: $$ \log(\mathcal{L})."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Likelihood and probability normalization, log-sum-exp trick"><meta name=twitter:description content="Working with probabilities involves multiplication and normalization of their values. Since the numerical values sometimes are extremely low that can lead to underflow problems. This problem is evident with particle filters - we have to multiply really low likelihood values that vanish in the end. Log-sum-exp allows to abbreviate this problem.
Approach Link to heading Log-likelihoods Link to heading Since the likelihood values can be extremely low it is more convenient to work with loglikelihood instead of likelihood: $$ \log(\mathcal{L})."><meta property="og:title" content="Likelihood and probability normalization, log-sum-exp trick"><meta property="og:description" content="Working with probabilities involves multiplication and normalization of their values. Since the numerical values sometimes are extremely low that can lead to underflow problems. This problem is evident with particle filters - we have to multiply really low likelihood values that vanish in the end. Log-sum-exp allows to abbreviate this problem.
Approach Link to heading Log-likelihoods Link to heading Since the likelihood values can be extremely low it is more convenient to work with loglikelihood instead of likelihood: $$ \log(\mathcal{L})."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/likelihood-and-log-sum-exp/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-11T23:00:00+03:00"><meta property="article:modified_time" content="2023-08-11T23:00:00+03:00"><link rel=canonical href=https://mikoff.github.io/posts/likelihood-and-log-sum-exp/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li><li class=navigation-item><a class=navigation-link href=/notes/>Notes</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://mikoff.github.io/posts/likelihood-and-log-sum-exp/>Likelihood and probability normalization, log-sum-exp trick</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2023-08-11T23:00:00+03:00>August 11, 2023
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/probability/>Probability</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/particle-filter/>Particle Filter</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/log-likelihood/>Log-Likelihood</a></span></div></div></header><div class=post-content><p>Working with probabilities involves multiplication and normalization of their values. Since the numerical values sometimes are extremely low that can lead to underflow problems.
This problem is evident with particle filters - we have to multiply really low likelihood values that vanish in the end. <em>Log-sum-exp</em> allows to abbreviate this problem.</p><h2 id=approach>Approach
<a class=heading-link href=#approach><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=log-likelihoods>Log-likelihoods
<a class=heading-link href=#log-likelihoods><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Since the likelihood values can be extremely low it is more convenient to work with loglikelihood instead of likelihood:</p>$$
\log(\mathcal{L}).
$$<p>Now, if we have independent measurements $i=1,\dots,n$ and associated likelihoods $\mathbf{L} = [\mathcal{L}_1, \dots, \mathcal{L}_n]$ we can calculate the joint likelihood as follows:</p>$$
w = \prod\mathcal{L}_i
$$<p>that is, obviously, numerically unstable, or joint log-likelihood:</p>$$
\tau = \log\prod\mathcal{L}_{i} = \log(\mathcal{L}_1\cdot \mathcal{L}_2\cdot ~\dots~ \cdot\mathcal{L}_n) = \log{\mathcal{L}_1} + \log{\mathcal{L}_2}+~\dots~+\log{\mathcal{L}_n} = \sum_{i=1}^n\log\mathcal{L}_i,
$$<p>by doing so we convert product to sum that doesn&rsquo;t lead to numerical underflow issue.</p><p>Quite often we are interested in relative likelihoods, or odds (since we normalize the probability afterwards) for our particles. Assuming that each particle has certain weight, all the weights, or log-weights are stacked into the vector:</p>$$
\begin{gather}
\mathbf{w} = [w_1,\dots, w_m],\\\\
\boldsymbol{\tau} = [\tau_1, \dots, \tau_m]
\end{gather}
$$<p>where $m$ is the number of particles.</p><p>To calculate the relative weight for each particle $j$ we can rewrite the equation:</p>$$
\begin{aligned}
\hat{w}_j &= \frac{w_j}{\max(\mathbf{w})}\\\\ &= \exp\log{\frac{w_j}{\max(\mathbf{w})}}\\\\ &= \exp\left( \log w_j - \max(\log\mathbf{w}) \right)\\\\ &= \exp(\tau_j - \max\boldsymbol{\tau})\\\\ &= \exp\left(\sum_{i=1}^n\log\mathcal{L}_i - \max\boldsymbol{\tau}\right).
\end{aligned}
$$<h3 id=probability-normalization>Probability normalization
<a class=heading-link href=#probability-normalization><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Now, having these <em>relative</em> weights, we can normalize them more conveniently:</p>$$
p_j = \frac{\hat{w}_j}{\sum_{j=1}^m \hat{w}_j},~\sum_{j=1}^m{p_j} = 1.
$$<p>Taking the logarithms on both side:</p>$$
\log p_i = \log\hat{w}_j - \log\sum_{j=1}^m\hat{w}_j,
$$<p>and making the aforementioned substitution $\hat{w}_j = \exp(\tau_j - \max\boldsymbol{\tau})$, after some rewriting we end up with:</p>$$
\begin{align}
\log p_i &= \log\exp(\tau_j - \max\boldsymbol{\tau}) - \log\sum\exp(\tau_j - \max\boldsymbol{\tau})\\\\
\log p_i &= \tau_j \underbrace{ - \max\boldsymbol{\tau} -\log\sum\exp(\tau_j - \max\boldsymbol{\tau})}_{\log\sum\exp(\boldsymbol{\tau})~-~\text{log-sum-exp trick}}
\\\\
p_i &= \exp\left( \tau_j - \max\boldsymbol{\tau} -\log\sum\exp(\tau_j - \max\boldsymbol{\tau}) \right)
\end{align}
$$<p>We ended up with a famous <em>log-sum-exp</em> trick<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, the second line from the end is the normalized log-weights, and the last one is the normalized weights. The <strong>underlying meaning</strong> of this trick is apparent now: it scales the weights before normalization, avoiding the numerical under- or overflow.</p><h3 id=effective-sample-size>Effective sample size
<a class=heading-link href=#effective-sample-size><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Quite often the following equation is used to estimate effective sample size:</p>$$
N_{eff} = \frac{1}{\sum_{j=1}^{m} p_j^2}.
$$<p>Since we work with log-likelihoods, substitute $p_j$ with $\exp\log p_j$:</p>$$
\begin{gather}
\log N_{eff} &= -\log\sum_{j=1}^m \exp\log p_j^2 \\\\
\log N_{eff} &= -\log\sum_{j=1}^m \exp(2\cdot\log p_j).
\end{gather}
$$<p>Resulting in the following equation for $N_{eff}$:</p>$$
N_{eff} = \exp\left(-\log\sum_{j=1}^m \exp(2\cdot\log p_i) \right).
$$<h2 id=code-examples>Code examples
<a class=heading-link href=#code-examples><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Assume that we have two likelihoods. By calculating the joint likelihood using multiplication, we, obviously, experience numerical underflow and end up with zeros:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> <span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> likelihoods1 <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>1e-249</span>, <span style=color:#099>1e-244</span>, <span style=color:#099>1e-244</span>, <span style=color:#099>1e-247</span>])
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> likelihoods2 <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array([<span style=color:#099>1e-249</span>, <span style=color:#099>1e-244</span>, <span style=color:#099>1e-244</span>, <span style=color:#099>1e-247</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> likelihoods1 <span style=color:#000;font-weight:700>*</span> likelihoods2
</span></span><span style=display:flex><span>array([<span style=color:#099>0.</span>, <span style=color:#099>0.</span>, <span style=color:#099>0.</span>, <span style=color:#099>0.</span>])
</span></span></code></pre></div><p>Let&rsquo;s calculate log-likelihoods and normalize probabilities using naive method:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> log_likelihood1 <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>log(likelihoods1)
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> log_likelihood2 <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>log(likelihoods2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> tau <span style=color:#000;font-weight:700>=</span> log_likelihood1 <span style=color:#000;font-weight:700>+</span> log_likelihood2
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> tau
</span></span><span style=display:flex><span>array([<span style=color:#000;font-weight:700>-</span><span style=color:#099>1132.87186575</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1123.66152538</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1123.66152538</span>, <span style=color:#000;font-weight:700>-</span><span style=color:#099>1137.47703594</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> np<span style=color:#000;font-weight:700>.</span>exp(tau)<span style=color:#000;font-weight:700>/</span>np<span style=color:#000;font-weight:700>.</span>sum(np<span style=color:#000;font-weight:700>.</span>exp(tau))
</span></span><span style=display:flex><span>array([nan, nan, nan, nan])
</span></span></code></pre></div><p>And now let&rsquo;s normalize using the derived equation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> max_tau <span style=color:#000;font-weight:700>=</span> tau<span style=color:#000;font-weight:700>.</span>max()
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> tau_normed <span style=color:#000;font-weight:700>=</span> tau <span style=color:#000;font-weight:700>-</span> max_tau <span style=color:#000;font-weight:700>-</span> np<span style=color:#000;font-weight:700>.</span>log(np<span style=color:#000;font-weight:700>.</span>sum(np<span style=color:#000;font-weight:700>.</span>exp(tau <span style=color:#000;font-weight:700>-</span> max_tau)))
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> w_normed <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>exp(tau_normed)
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> w_normed
</span></span><span style=display:flex><span>array([<span style=color:#099>0.00005</span>   , <span style=color:#099>0.49997475</span>, <span style=color:#099>0.49997475</span>, <span style=color:#099>0.0000005</span> ])
</span></span></code></pre></div><p>If we are interested in effective number of particles:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>logSumExp</span>(tau):
</span></span><span style=display:flex><span>    max_tau <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>max(tau)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> max_tau <span style=color:#000;font-weight:700>+</span> np<span style=color:#000;font-weight:700>.</span>log(np<span style=color:#000;font-weight:700>.</span>sum(np<span style=color:#000;font-weight:700>.</span>exp(tau <span style=color:#000;font-weight:700>-</span> max_tau)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> tau_normed <span style=color:#000;font-weight:700>=</span> tau <span style=color:#000;font-weight:700>-</span> logSumExp(tau)
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> effective_sample_size <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>exp(<span style=color:#000;font-weight:700>-</span>logSumExp(<span style=color:#099>2</span> <span style=color:#000;font-weight:700>*</span> tau_normed))
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>&gt;&gt;&gt;</span> effective_sample_size
</span></span><span style=display:flex><span><span style=color:#099>2.00020199509849</span>
</span></span></code></pre></div><p>Success! Works perfectly!</p><h2 id=references>References
<a class=heading-link href=#references><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://mc-stan.org/docs/2_27/stan-users-guide/log-sum-of-exponentials.html class=external-link target=_blank rel=noopener>https://mc-stan.org/docs/2_27/stan-users-guide/log-sum-of-exponentials.html</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2024
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>