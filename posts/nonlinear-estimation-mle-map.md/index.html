<!doctype html><html lang=en><head><title>Nonlinear estimation: Full Bayesian, MLE and MAP · Aleksandr Mikoff's blog
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Aleksandr Mikoff"><meta name=description content="Intro Link to heading Recently I have read &ldquo;State Estimation for Robotics&rdquo; book and came across a good example on one-dimensional nonlinear estimation problem: the estimation of the position of a landmark from stereo-camera data.
Distance from stereo-images Link to heading The camera image is a projection of the world on the image plane. The depth perceptions arises from disparity of 3d point (landmark) on two images, obtained from left and right cameras."><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Nonlinear estimation: Full Bayesian, MLE and MAP"><meta name=twitter:description content="Intro Link to heading Recently I have read &ldquo;State Estimation for Robotics&rdquo; book and came across a good example on one-dimensional nonlinear estimation problem: the estimation of the position of a landmark from stereo-camera data.
Distance from stereo-images Link to heading The camera image is a projection of the world on the image plane. The depth perceptions arises from disparity of 3d point (landmark) on two images, obtained from left and right cameras."><meta property="og:title" content="Nonlinear estimation: Full Bayesian, MLE and MAP"><meta property="og:description" content="Intro Link to heading Recently I have read &ldquo;State Estimation for Robotics&rdquo; book and came across a good example on one-dimensional nonlinear estimation problem: the estimation of the position of a landmark from stereo-camera data.
Distance from stereo-images Link to heading The camera image is a projection of the world on the image plane. The depth perceptions arises from disparity of 3d point (landmark) on two images, obtained from left and right cameras."><meta property="og:type" content="article"><meta property="og:url" content="https://mikoff.github.io/posts/nonlinear-estimation-mle-map.md/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-04-18T10:51:21+03:00"><meta property="article:modified_time" content="2020-04-18T10:51:21+03:00"><link rel=canonical href=https://mikoff.github.io/posts/nonlinear-estimation-mle-map.md/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/image.min.c1a5dfc6bac0eb1b85bcd8abf8aba0d18e0bf02fc972f9a0b17d2962f5ca8dd5.css integrity="sha256-waXfxrrA6xuFvNir+Kug0Y4L8C/JcvmgsX0pYvXKjdU=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/spoiler.min.bf901294afff95f520a8150a4df4249576eb9c49c4f40f5f9c2de750588dd594.css integrity="sha256-v5ASlK//lfUgqBUKTfQklXbrnEnE9A9fnC3nUFiN1ZQ=" crossorigin=anonymous media=screen><link rel=stylesheet href=/plugins/academic-icons/css/academicons.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://mikoff.github.io/>Aleksandr Mikoff's blog
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/tags>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://mikoff.github.io/posts/nonlinear-estimation-mle-map.md/>Nonlinear estimation: Full Bayesian, MLE and MAP</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2020-04-18T10:51:21+03:00>April 18, 2020
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
8-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/bayesian-estimation/>Bayesian Estimation</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/mle/>MLE</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/map/>MAP</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/stereo-vision/>Stereo Vision</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/statistics/>Statistics</a></span></div></div></header><div class=post-content><h2 id=intro>Intro
<a class=heading-link href=#intro><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Recently I have read <a href=https://www.cambridge.org/core/books/state-estimation-for-robotics/AC0E0AC229C55203B3C8F106BCB61F48 class=external-link target=_blank rel=noopener>&ldquo;State Estimation for Robotics&rdquo;</a> book and came across a good example on one-dimensional nonlinear estimation problem: the estimation of the position of a landmark from stereo-camera data.</p><h2 id=distance-from-stereo-images>Distance from stereo-images
<a class=heading-link href=#distance-from-stereo-images><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The camera image is a projection of the world on the image plane.
The depth perceptions arises from disparity of 3d point (landmark) on two images, obtained from left and right cameras.</p>$$disparity = x_{left} - x_{right}$$<p><img src=./two_camera_images.svg#center alt=svg></p><p>Having two images with known landmark positions in image coordinates (pixels), it turns out that we can calculate the depth between camera and the landmark.</p><p>To do so, let&rsquo;s draw a simple scheme of two pinholes cameras&rsquo; setup and derive the equation for depth, or forward distance, between the camera and the landmark.</p><p><img src=./camera_model_only.svg#center alt=svg></p><p>On the preceding image $l$ is the landmark, $(x, z)$ is the landmark position w.r.t. the left camera $c_{left}$ (meters). We want to find $z$, depth of the landmark. By design, the focal length $f$ (in pixels) and baseline $b$, which is the horizontal distance between left and right cameras (in meters) are known.</p><p>From the similar triangles (marked with arcs) and under assumption that the optical axes of the cameras are parallel we have (for simplicty $x_{left} = x_{l}, x_{right} = x_r$):</p>$$\frac{z}{f} = \frac{x}{x_{l}} = \frac{x-b}{x_{r}}.$$<p>Now, expressing $x$ in terms of $x_{l}$ and $x_{r}$ gives:</p>$$x x_{r} = (x - b) x_{l}$$
$$x (x_{r} - x_{l}) = -b x_{l}$$
$$x = \frac{b x_{l}}{x_{l} - x_{r}}$$<p>Substituting it into previous equation:</p>$$\frac{z}{f} = \frac{b}{x_r - x_l}$$<p>we can derive the equation for depth, or $z$ as:</p>$$z = \frac{f b}{x_l - x_r}.$$<h2 id=bayesian-model>Bayesian model
<a class=heading-link href=#bayesian-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>From the depth equation the measurement model for disparity $disparity = x_l - x_r$ can be written as:</p>$$disparity = x_l - x_r = y = \frac{f b}{z} + n,$$<p>where the state $z$ is the depth of the landmark (meters) and $n$ is the measurement noise (in pixels), all other terms were defined earlier. Despite of being simple it is a nonlinear model: $f(x_1 + x_2) \neq f(x_1) + f(x_2)$.</p><p>This fact can be visualized by passing the state through our measurement function. After applying this transform evenly spaced samples from our state space become unevenly spaced:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>10</span>, <span style=color:#099>3</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># depth</span>
</span></span><span style=display:flex><span>z_space <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(<span style=color:#099>1e-8</span>, <span style=color:#099>40</span>, <span style=color:#099>100</span>)
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># corresponding disparity measurements</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>measurement_space <span style=color:#000;font-weight:700>=</span> <span style=color:#099>400</span> <span style=color:#000;font-weight:700>*</span> <span style=color:#099>0.1</span> <span style=color:#000;font-weight:700>/</span> z_space
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines(z_space, <span style=color:#099>0</span>, <span style=color:#099>0.2</span>, colors<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;k&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;state samples, $z$&#34;</span>, lw <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.5</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines(measurement_space, <span style=color:#099>0</span>, <span style=color:#099>0.2</span>, colors<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;green&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;measurement samples, $y$&#34;</span>, lw <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.5</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set_xlim(<span style=color:#099>0</span>, <span style=color:#099>40</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>set_ylim(<span style=color:#000;font-weight:700>-</span><span style=color:#099>0.2</span>, <span style=color:#099>0.4</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span></code></pre></div><p><img src=./index_9_1.png alt=png></p><p>The Bayesian update is defined as:</p>$$p(z|y) = \eta~p(y|z)~p(z),$$<p>where $\eta = \frac{1}{p(y)} = \frac{1}{\int p(y|z)~p(z)~dz}$ is the normalizer.</p><p>To perform such update, the probabilities of $p(y|z)$ and $p(z)$ must be known. Our choice is to define the measurement noise as zero-mean Gaussian $n \sim N(0, R)$. Thus:</p>$$p(y|z) = N\left(\frac{f b}{z}, R\right) = \frac{1}{\sqrt{2 \pi R}} exp\left(-\frac{1}{2 R} \left(y - \frac{f b}{z}\right)^2\right).$$<p>We also asssume that the prior is Gaussian:</p>$$p(z) = N(z^{-}, P^{-}) = \frac{1}{\sqrt{2 \pi P^{-}}} exp\left(-\frac{1}{2 P^{-}} (z - z^{-})^2\right).$$<p>The Bayesian framework consists of the following operations:</p><ol><li>Get the prior $p(z)$ over the state space $z$, given the current estimate $z^{-}$.</li><li>Obtain the measurement $y$.</li><li>Calculate the measurement likelihood using $p(y|z)$.</li><li>Calculate the posterior $p(z|y)$ using Bayes rule.</li></ol><h2 id=visualization>Visualization
<a class=heading-link href=#visualization><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To illustrate how this nonlinearity affects the pdf of the posterior it is possible to perform numerical experiments. For such an experiment, the selected parameters are the following:</p>$$z^{-} = 20~[m],~P^{-} = 9~[m^2],~f = 400~[pixel],~b = 0.1~[m],~R = 0.09~[pixel^2].$$<p>The parameters for posterior calculation:</p>$$z_{true} = 22~[m],~y_{meas} = \frac{f b}{z_{true}} + 1~[pixel].$$<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># prior parameters</span>
</span></span><span style=display:flex><span>z_prior <span style=color:#000;font-weight:700>=</span> <span style=color:#099>20</span>
</span></span><span style=display:flex><span>P_prior <span style=color:#000;font-weight:700>=</span> <span style=color:#099>9</span>
</span></span><span style=display:flex><span>f <span style=color:#000;font-weight:700>=</span> <span style=color:#099>400</span>
</span></span><span style=display:flex><span>b <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.1</span>
</span></span><span style=display:flex><span>R <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.09</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># measurement parameters</span>
</span></span><span style=display:flex><span>z_true <span style=color:#000;font-weight:700>=</span> <span style=color:#099>22</span>
</span></span><span style=display:flex><span>noise <span style=color:#000;font-weight:700>=</span> <span style=color:#099>1.0</span>
</span></span><span style=display:flex><span>y_meas <span style=color:#000;font-weight:700>=</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z_true <span style=color:#000;font-weight:700>+</span> noise
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>15</span>, <span style=color:#099>4</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># prior</span>
</span></span><span style=display:flex><span>z_space <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>linspace(<span style=color:#099>1e-8</span>, <span style=color:#099>40</span>, <span style=color:#099>10000</span>)
</span></span><span style=display:flex><span>priors <span style=color:#000;font-weight:700>=</span> stats<span style=color:#000;font-weight:700>.</span>norm<span style=color:#000;font-weight:700>.</span>pdf(z_space, loc <span style=color:#000;font-weight:700>=</span> z_prior, scale <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>sqrt(P_prior))
</span></span><span style=display:flex><span>priors <span style=color:#000;font-weight:700>=</span> priors <span style=color:#000;font-weight:700>/</span> priors<span style=color:#000;font-weight:700>.</span>sum()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(z_space, priors, <span style=color:#d14>&#39;r-&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;prior&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># likelihood</span>
</span></span><span style=display:flex><span>measurement_space <span style=color:#000;font-weight:700>=</span> <span style=color:#099>400</span> <span style=color:#000;font-weight:700>*</span> <span style=color:#099>0.1</span> <span style=color:#000;font-weight:700>/</span> z_space
</span></span><span style=display:flex><span>likelihoods <span style=color:#000;font-weight:700>=</span> stats<span style=color:#000;font-weight:700>.</span>norm<span style=color:#000;font-weight:700>.</span>pdf(measurement_space, loc <span style=color:#000;font-weight:700>=</span> y_meas, scale <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>sqrt(R))
</span></span><span style=display:flex><span>likelihoods <span style=color:#000;font-weight:700>=</span> likelihoods <span style=color:#000;font-weight:700>/</span> likelihoods<span style=color:#000;font-weight:700>.</span>sum()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(z_space, likelihoods, <span style=color:#d14>&#39;g--&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;likelihood&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># posterior</span>
</span></span><span style=display:flex><span>posteriors <span style=color:#000;font-weight:700>=</span> priors <span style=color:#000;font-weight:700>*</span> likelihoods
</span></span><span style=display:flex><span>posteriors <span style=color:#000;font-weight:700>=</span> posteriors <span style=color:#000;font-weight:700>/</span> np<span style=color:#000;font-weight:700>.</span>sum(posteriors)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>plot(z_space, posteriors, <span style=color:#d14>&#39;b-.&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;posterior&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ymin, ymax <span style=color:#000;font-weight:700>=</span> ax<span style=color:#000;font-weight:700>.</span>get_ylim()
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines([z_true], ymin, ymax, colors <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;magenta&#39;</span>, linestyles <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;dotted&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$z_</span><span style=color:#d14>{true}</span><span style=color:#d14>$&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>zFromMeas <span style=color:#000;font-weight:700>=</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> y_meas
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines([zFromMeas], ymin, ymax, colors <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;brown&#39;</span>, linestyles <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;dotted&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$g^{-1}(y_</span><span style=color:#d14>{meas}</span><span style=color:#d14>)$&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>maxPosterior <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>argmax(posteriors)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines([z_space[maxPosterior]], ymin, ymax, colors <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;blue&#39;</span>, linestyles <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;dotted&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$z^{+}$&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span></code></pre></div><p><img src=./index_14_1.png alt=png></p><p>The maximum likelihood estimation (MLE) can be utilized when we want to use only the measurements and have no prior information about possible system state. It helps to find the answer to the following question: which value of $z$ is most probable given the measurement $y$?</p><p>However, if we know the prior distribution of our parameter of interest, it is generally a good idea to take it into account. The incorporation of the measurement results in more concentrated (less uncertainty) posterior about the state than the prior.</p><p>We can find the most probable posterior value for this concrete example by visual inspection of the posterior PDF. However, it is not the case for most real-life problems. To overcome this problem the maximum a posteriori (MAP) approach can be utilized. It is used to find the <em>peak</em> of the posterior.</p><h2 id=maximum-likelihood-estimation>Maximum Likelihood estimation
<a class=heading-link href=#maximum-likelihood-estimation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Talking about the maximum likelihood estimation we seek for a value that maximizes the likelihood of our measurement, or data, given the state $z$:</p>$$\hat{x}_{mle} = \underset{x}{arg~max}~p(y|x),$$<p>going to the $log$ space:</p>$$\hat{x}_{mle} = \underset{x}{arg~min}~(-ln(p(y|x))).$$<p><em>Note</em>: if we want to find MLE for a number of measurements, or samples under assumption that they are undepended, we replace $p(y|x)$ by the product of individual samples probabilities: $\underset{i}{\prod} p(y_i|x)$. This leads to the following $\hat{x}_{mle}$:</p>$$\hat{x}_{mle} = \underset{x}{arg~min}~(-\underset{i}{\sum}ln(p(y_i|x))).$$<p>For stereo-camera depth estimate:</p>$$\hat{z}_{mle} = \underset{z}{arg~min}~J_{mle}(z),$$<p>where $J_{mle}(z)$ can be found as:</p>$$J_{mle}(z) = \frac{1}{2 R} \left(y - \frac{f b}{z}\right)^2,$$<p>after dropping the constants that do not depend on $x$.</p><h2 id=maximum-a-posteriori-estimation>Maximum a Posteriori estimation
<a class=heading-link href=#maximum-a-posteriori-estimation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>When we are talking about maximum a posteriori estimation we assume that we want to find the single state value $x$ that maximizes the true posterior:</p>$$\hat{x}_{map} = \underset{x}{arg~max}~p(x|y),$$<p>or, alternatively, because $log$ is the monotolically increasing function:</p>$$\hat{x}_{map} = \underset{x}{arg~min}~(-ln(p(x|y))).$$<p>Applying the Bayes rule we have:</p>$$\hat{x}_{map} = \underset{x}{arg~min}~(-ln(p(y|x)) - ln(p(x))),$$<p>where $p(y)$ is being dropped because it does not depend on $x$. Again, if we have a number of measurements $y_i$, we have to add their product that leads to the sum after switching to the $log$ space.</p><p>In the stereo-camera depth estimate we can write:</p>$$\hat{z}_{map} = \underset{z}{arg~min}~J_{map}(z),$$<p>where $J(z)$ is obtained by dropping the constants that do not depend on $x$:</p>$$J_{map}(z) = \frac{1}{2 R} \left(y - \frac{f b}{z}\right)^2 + \frac{1}{2 P^{-}} \left(z^{-} - z\right)^2$$<p>Both MLE and MAP estimates can be found using a number of numerical optimization techniques by minimizing our target cost functions $J_{mle}$ or $J_{map}$.</p><p>Let&rsquo;s go back and find the MLE and MAP estimates of $z$:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>scipy.optimize</span> <span style=color:#000;font-weight:700>import</span> minimize
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Jz_mle <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>lambda</span> z: <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> R) <span style=color:#000;font-weight:700>*</span> (y_meas <span style=color:#000;font-weight:700>-</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ml_estimate <span style=color:#000;font-weight:700>=</span> minimize(Jz_mle, <span style=color:#099>1e-8</span>)<span style=color:#000;font-weight:700>.</span>x[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;ML estimate:&#34;</span>, ml_estimate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Jz_map <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>lambda</span> z: <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> R) <span style=color:#000;font-weight:700>*</span> (y_meas <span style=color:#000;font-weight:700>-</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span> <span style=color:#000;font-weight:700>+</span> \
</span></span><span style=display:flex><span>        <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> P_prior) <span style=color:#000;font-weight:700>*</span> (z_prior <span style=color:#000;font-weight:700>-</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>map_estimate <span style=color:#000;font-weight:700>=</span> minimize(Jz_map, <span style=color:#099>1e-8</span>)<span style=color:#000;font-weight:700>.</span>x[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;MAP estimate: &#34;</span>, map_estimate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines([ml_estimate], ymin, ymax, colors <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;green&#39;</span>, linestyles <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;-&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$z_</span><span style=color:#d14>{ml}</span><span style=color:#d14>$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>vlines([map_estimate], ymin, ymax, colors <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;blue&#39;</span>, linestyles <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;-&#39;</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;$z_</span><span style=color:#d14>{map}</span><span style=color:#d14>$&#39;</span>)
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>IPython<span style=color:#000;font-weight:700>.</span>display<span style=color:#000;font-weight:700>.</span>display(fig)
</span></span></code></pre></div><pre><code>ML estimate: 14.19354712952584
MAP estimate:  15.671431302257346
</code></pre><p><img src=./index_20_1.png alt=png></p><p>To test whether or not the estimators find the true value of $z$ the simulations can be performed, estimating the residual between true and estimated states.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=color:#000;font-weight:700>=</span> {<span style=color:#d14>&#39;true_state&#39;</span> : [], <span style=color:#d14>&#39;measurement&#39;</span> : [], <span style=color:#d14>&#34;map_estimate&#34;</span> : [], <span style=color:#d14>&#34;mle_estimate&#34;</span> : []}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>0</span>, <span style=color:#099>10000</span>):
</span></span><span style=display:flex><span>    z_true <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>random<span style=color:#000;font-weight:700>.</span>normal(z_prior, np<span style=color:#000;font-weight:700>.</span>sqrt(P_prior))
</span></span><span style=display:flex><span>    y_meas <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>random<span style=color:#000;font-weight:700>.</span>normal(f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z_true, np<span style=color:#000;font-weight:700>.</span>sqrt(R))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    Jz_mle <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>lambda</span> z: <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> R) <span style=color:#000;font-weight:700>*</span> (y_meas <span style=color:#000;font-weight:700>-</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>
</span></span><span style=display:flex><span>    mle_estimate <span style=color:#000;font-weight:700>=</span> minimize(Jz_mle, <span style=color:#099>1e-8</span>)<span style=color:#000;font-weight:700>.</span>x[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    Jz_map <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>lambda</span> z: <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> R) <span style=color:#000;font-weight:700>*</span> (y_meas <span style=color:#000;font-weight:700>-</span> f <span style=color:#000;font-weight:700>*</span> b <span style=color:#000;font-weight:700>/</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span> <span style=color:#000;font-weight:700>+</span> \
</span></span><span style=display:flex><span>        <span style=color:#099>1.0</span> <span style=color:#000;font-weight:700>/</span> (<span style=color:#099>2.0</span> <span style=color:#000;font-weight:700>*</span> P_prior) <span style=color:#000;font-weight:700>*</span> (z_prior <span style=color:#000;font-weight:700>-</span> z) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>
</span></span><span style=display:flex><span>    map_estimate <span style=color:#000;font-weight:700>=</span> minimize(Jz_map, <span style=color:#099>1e-8</span>)<span style=color:#000;font-weight:700>.</span>x[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results[<span style=color:#d14>&#39;true_state&#39;</span>]<span style=color:#000;font-weight:700>.</span>append(z_true)
</span></span><span style=display:flex><span>    results[<span style=color:#d14>&#39;mle_estimate&#39;</span>]<span style=color:#000;font-weight:700>.</span>append(mle_estimate)
</span></span><span style=display:flex><span>    results[<span style=color:#d14>&#39;map_estimate&#39;</span>]<span style=color:#000;font-weight:700>.</span>append(map_estimate)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>true_results <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array(results[<span style=color:#d14>&#39;true_state&#39;</span>])
</span></span><span style=display:flex><span>mle_results <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array(results[<span style=color:#d14>&#39;mle_estimate&#39;</span>])
</span></span><span style=display:flex><span>map_results <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>array(results[<span style=color:#d14>&#39;map_estimate&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig, ax <span style=color:#000;font-weight:700>=</span> plt<span style=color:#000;font-weight:700>.</span>subplots(<span style=color:#099>1</span>, <span style=color:#099>1</span>, figsize <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>15</span>, <span style=color:#099>4</span>))
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>hist(mle_results, bins<span style=color:#000;font-weight:700>=</span><span style=color:#099>50</span>, alpha <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.5</span>, density <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>True</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;MLE estimate&#34;</span>);
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>hist(map_results, bins<span style=color:#000;font-weight:700>=</span><span style=color:#099>50</span>, alpha <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0.5</span>, density <span style=color:#000;font-weight:700>=</span> <span style=color:#000;font-weight:700>True</span>, label <span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#34;MAP estimate&#34;</span>);
</span></span><span style=display:flex><span>ax<span style=color:#000;font-weight:700>.</span>legend()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Mean error MAP and true state:&#34;</span>, np<span style=color:#000;font-weight:700>.</span>mean(map_results <span style=color:#000;font-weight:700>-</span> true_results))
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Variance of the error: &#34;</span>, np<span style=color:#000;font-weight:700>.</span>mean((map_results  <span style=color:#000;font-weight:700>-</span> true_results) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Mean error MLE and true state:&#34;</span>, np<span style=color:#000;font-weight:700>.</span>mean(mle_results <span style=color:#000;font-weight:700>-</span> true_results))
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Variance of the error: &#34;</span>, np<span style=color:#000;font-weight:700>.</span>mean((mle_results  <span style=color:#000;font-weight:700>-</span> true_results) <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>))
</span></span></code></pre></div><pre><code>Mean error MAP and true state: -0.3708958146195719
Variance of the error:  4.335754086406355
Mean error MLE and true state: 0.475211450081065
Variance of the error:  13.431400451884032
</code></pre><p><img src=./index_22_1.png alt=png></p><p>When selecting between MLE and MAP estimators it is a good practice to use MAP when the prior is given or can be inferred from experiments or researcher&rsquo;s intuition. Also, it is important to note that if the prior is a uniform distribution, MAP becomes an equivalent to MLE.</p><p>Both MLE and MAP estimators are biased even for such vanilla example. Why it happens and what to do with that?
There is no straightforward answer: it depends on the problem. One of approaches that tackle this problem is the Bias-Variance decomposition.</p><p>Jupyter notebook is available <a href=https://github.com/mikoff/blog_projects/blob/master/notebooks/Nonlinear_estimation_of_stereo-vision_problem_using_MLE_and_MAP/ class=external-link target=_blank rel=noopener>here</a>.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mikoff-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2024
Aleksandr Mikoff
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>