<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Probability on Aleksandr Mikoff's blog</title><link>https://mikoff.github.io/tags/probability/</link><description>Recent content in Probability on Aleksandr Mikoff's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 24 Mar 2024 21:00:00 +0300</lastBuildDate><atom:link href="https://mikoff.github.io/tags/probability/index.xml" rel="self" type="application/rss+xml"/><item><title>MCMC sampling</title><link>https://mikoff.github.io/posts/mcmc-sampling.md/</link><pubDate>Sun, 24 Mar 2024 21:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/mcmc-sampling.md/</guid><description>Quite often we want to sample from distributions that have computationally untractable CDF. To draw samples from them the numerical procedures are used. In the following note I would like to demonstrate few approaches for one particular example: having 2D robot perception and a map we want to sample most probable poses of this robot. This problem often emerges during initialization or re-initialization of the estimated robot pose when the filter diverged or needs to be initialized from scratch and we want to guarantee the fast convergence.</description></item><item><title>Probability density transform</title><link>https://mikoff.github.io/posts/probability-density-transform/</link><pubDate>Sat, 27 Jan 2024 12:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/probability-density-transform/</guid><description>PDF transformations Link to heading While reading new [book]1 by Bishop I came across pdf transformation topic. It turned out to be counter-intuitive that we need not only transform the pdf by the selected function, but also multiply it by the derivative of the inverse function w.r.t. substituted variable. While delving into the details of this topic, I found the following sources to be quite useful: [2]2, [3]3, [4]4, [5]5.</description></item><item><title>Likelihood and probability normalization, log-sum-exp trick</title><link>https://mikoff.github.io/posts/likelihood-and-log-sum-exp/</link><pubDate>Fri, 11 Aug 2023 23:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/likelihood-and-log-sum-exp/</guid><description>Working with probabilities involves multiplication and normalization of their values. Since the numerical values sometimes are extremely low that can lead to underflow problems. This problem is evident with particle filters - we have to multiply really low likelihood values that vanish in the end. Log-sum-exp allows to abbreviate this problem.
Approach Link to heading Log-likelihoods Link to heading Since the likelihood values can be extremely low it is more convenient to work with loglikelihood instead of likelihood: $$ \log(\mathcal{L}).</description></item></channel></rss>