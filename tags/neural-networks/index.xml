<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>neural networks on Aleksandr Mikoff's blog</title><link>https://mikoff.github.io/tags/neural-networks/</link><description>Recent content in neural networks on Aleksandr Mikoff's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 15 Oct 2023 12:00:00 +0300</lastBuildDate><atom:link href="https://mikoff.github.io/tags/neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding deep learning: training, SGD, code samples</title><link>https://mikoff.github.io/posts/nn-training.md/</link><pubDate>Sun, 15 Oct 2023 12:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/nn-training.md/</guid><description>Recently, I have been reading a new [book]1 by S. Prince titled &amp;ldquo;Understanding Deep Learning.&amp;rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD.</description></item></channel></rss>