<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep learning on Aleksandr Mikoff's blog</title><link>https://mikoff.github.io/tags/deep-learning/</link><description>Recent content in deep learning on Aleksandr Mikoff's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 15 Oct 2023 12:00:00 +0300</lastBuildDate><atom:link href="https://mikoff.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding deep learning: training, SGD, code samples</title><link>https://mikoff.github.io/posts/nn-training.md/</link><pubDate>Sun, 15 Oct 2023 12:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/nn-training.md/</guid><description>Recently, I have been reading a new [book]1 by S. Prince titled &amp;ldquo;Understanding Deep Learning.&amp;rdquo; While reading it, I made some notes and practiced with concepts that were described in great detail by the author. Having no prior experience in deep learning, I was fascinated by how clearly the author explains the concepts and main terms.
This post is:
a collection of keynotes from the first seven chapters of the book, that I have found useful for myself; the numpy-only implementation of deep neural network with variable layers size and training using SGD.</description></item><item><title>Notes on backpropagation</title><link>https://mikoff.github.io/posts/notes-on-backpropagation/</link><pubDate>Sat, 19 Feb 2022 23:00:00 +0300</pubDate><guid>https://mikoff.github.io/posts/notes-on-backpropagation/</guid><description>Notes on backpropagation In optimization and machine learning applications the widely used tool for finding the model parameters is the gradient descent. It allows to find the maximum or minimum of the target function w.r.t. the parameters, in other words, to minimize the discrepancy between the model and the data.
However, to use this method the gradient has to be computed. The first problem is that if our function has a complex form the process of differentiating is quite tricky.</description></item></channel></rss>